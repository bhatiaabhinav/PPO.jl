using MDPs
import MDPs: preexperiment, postepisode, poststep
using UnPack
using Random
using Flux
using Flux.Zygote
using Flux: Optimiser, ClipNorm

export PPOLearner

Base.@kwdef struct PPOLearner <: AbstractHook
    envs::Vector{AbstractMDP}
    actor
    critic
    Î³::Float32 = 0.99
    horizon::Int
    ent_bonus::Float32 = 0.01f0
    clipnorm = Inf
    actor_lr = 0.0003
    critic_lr = 0.001
    optim_actor = Adam(actor_lr)
    optim_critic = clipnorm < Inf ? Optimiser(ClipNorm(clipnorm), Adam(critic_lr)) : Adam(critic_lr)
    train_interval = horizon
    tbptt_horizon = 4
    gradsteps = 1
    Î» = 0.95f0
    Ïµ = 0.2f0
    kl_target = 0.01
    max_actor_iters = 20
    ppo = true
    device = cpu
    actor_gpu = device(actor)
    critic_gpu = device(critic)
    stats = Dict{Symbol, Any}()
end


function poststep(a2cl::PPOLearner; returns, steps, rng, kwargs...)
    @unpack actor, critic, actor_gpu, critic_gpu, envs, horizon, Î³, ent_bonus, optim_actor, optim_critic, train_interval, tbptt_horizon, gradsteps, Î», Ïµ, kl_target, max_actor_iters, stats, ppo, device = a2cl
    isrecurrent = actor isa RecurrentActor

    n = size(action_space(envs[1]), 1)
    nenvs = length(envs)

    function train_actor_critic()

        # collect data using synchronous parallel actors
        function collect_trajectories()
            Flux.reset!(actor)
            Flux.reset!(critic)
            for env in envs
                reset!(env; rng=rng)
            end
            ð¬â‚œ = Zygote.@ignore mapreduce(state, hcat, envs) |> tof32
            if isrecurrent
                ð¬â‚œ = Zygote.@ignore vcat(zeros(Float32, n, nenvs), zeros(Float32, 1, nenvs), ð¬â‚œ, zeros(Float32, 1, nenvs))
            end
            ð¯â‚œ = critic(ð¬â‚œ)
            data = map(1:horizon) do t
                ð›‘â‚œ, _ = get_probs_logprobs(actor, ð¬â‚œ)
                ðšâ‚œ = mapreduce(hcat, 1:nenvs) do i  # 1-TODO: Can be parallelized
                    aáµ¢â‚œ = sample(rng, 1:n, ProbabilityWeights(ð›‘â‚œ[:, i]))
                    in_absorbing_state(envs[i]) ? reset!(envs[i]; rng=rng) : step!(envs[i], aáµ¢â‚œ; rng=rng)
                    return CartesianIndex(aáµ¢â‚œ, i)
                end
                ð«â‚œ = mapreduce(reward, hcat, envs) |> tof32
                ð¬â‚œâ€²= mapreduce(state, hcat, envs) |> tof32
                ðâ‚œ = mapreduce(in_absorbing_state, hcat, envs) |> tof32
                if isrecurrent
                    aonehot = zeros(Float32, n, nenvs)
                    aonehot[ðšâ‚œ] .= 1
                    ð¬â‚œâ€² = vcat(aonehot, ð«â‚œ, ð¬â‚œâ€², ðâ‚œ)
                end
                ð¯â‚œâ€² = critic(ð¬â‚œâ€²)
                # @info "hey" nenvs size(ð«â‚œ) size(ðâ‚œ) size(ð¯â‚œ) size(ð¯â‚œâ€²)
                ð›…â‚œ = ð«â‚œ + Î³ * (1f0 .- ðâ‚œ) .* ð¯â‚œâ€² - ð¯â‚œ
                dataâ‚œ = (ð¬â‚œ, ðšâ‚œ, ð«â‚œ, ðâ‚œ, ð¬â‚œâ€², ð›…â‚œ, ð›‘â‚œ, ð¯â‚œ)
                ð¬â‚œ = ð¬â‚œâ€²
                ð¯â‚œ = ð¯â‚œâ€²
                return dataâ‚œ
            end
            return data
        end
        
        # update advantages using GAE
        function update_advantates!(data)
            ð€â‚œâ€² = 0
            for dataâ‚œ in reverse(data)
                (ð¬â‚œ, ðšâ‚œ, ð«â‚œ, ðâ‚œ, ð¬â‚œâ€², ð›…â‚œ, ð›‘â‚œ, ð¯â‚œ) = dataâ‚œ
                ð›…â‚œ .+= Î³ * Î» * (1f0 .- ðâ‚œ) .* ð€â‚œâ€²
                ð€â‚œâ€² = ð›…â‚œ
            end
        end
        
        # update critic. Truncated backprop through time
        function update_critic!(critic, data)
            vÌ„ = 0
            Ï• = Flux.params(critic)
            Flux.reset!(critic);
            â„“Ï• = mapreduce(+, splitequal(horizon, tbptt_horizon)) do timechunk
                loss, âˆ‡ = withgradient(Ï•) do
                    critic_loss = 0f0
                    for (ð¬â‚œ, ðšâ‚œ, ð«â‚œ, ðâ‚œ, ð¬â‚œâ€², ð›…â‚œ, ð›‘â‚œ, ð¯â‚œ) in data[timechunk]
                        critic_loss += Flux.mse(critic(ð¬â‚œ), ð¯â‚œ + ð›…â‚œ)
                        vÌ„ += mean(ð¯â‚œ)
                    end
                    return critic_loss
                end
                Flux.update!(optim_critic, Ï•, âˆ‡)
                return loss
            end
            return (â„“Ï•, vÌ„) ./ horizon
        end

        # update actor. Truncated backprop through time. Uses PPO-clip objective if `ppo` is true
        function update_actor!(actor, data)
            HÌ„, kl_div = 0, 0
            Î¸ = Flux.params(actor)
            # update actor. Need a full forward rollout if want to handle recurrent network
            Flux.reset!(actor)
            â„“Î¸ = mapreduce(+, splitequal(horizon, tbptt_horizon)) do timechunk
                loss, âˆ‡ = withgradient(Î¸) do
                    actor_loss = 0f0
                    for (ð¬â‚œ, ðšâ‚œ, ð«â‚œ, ðâ‚œ, ð¬â‚œâ€², ð€â‚œ, oldð›‘â‚œ, ð¯â‚œ) in data[timechunk]
                        ð›‘â‚œ, logð›‘â‚œ = get_probs_logprobs(actor, ð¬â‚œ)
                        if ppo
                            ð‘Ÿâ‚œ = ð›‘â‚œ[ðšâ‚œ] ./ oldð›‘â‚œ[ðšâ‚œ]
                            actor_loss += -min.(ð‘Ÿâ‚œ .* ð€â‚œ, clamp.(ð‘Ÿâ‚œ, 1-Ïµ, 1+Ïµ) .* ð€â‚œ) |> mean
                        else
                            actor_loss -= ð€â‚œ .* logð›‘â‚œ[ðšâ‚œ] |> mean
                        end
                        HÌ„â‚œ = -sum(ð›‘â‚œ .* logð›‘â‚œ; dims=1) |> mean
                        actor_loss -= ent_bonus * HÌ„â‚œ
                        HÌ„ += HÌ„â‚œ
                        kl_div += kldivergence(oldð›‘â‚œ, ð›‘â‚œ)
                    end
                    return actor_loss
                end
                Flux.update!(optim_actor, Î¸, âˆ‡)
                return loss
            end
            return (â„“Î¸, HÌ„, kl_div) ./ horizon
        end

        function update_actor_multiple_iters!(actor, data, iters)
            (â„“Î¸, HÌ„, kl_div) = 0, 0, 0
            num_iters = 0
            for iter in 1:iters
                â„“Î¸, HÌ„, kl_div = update_actor!(actor, data)
                num_iters += 1
                kl_div >= kl_target && break
            end
            return â„“Î¸, HÌ„, kl_div, num_iters
        end

        data = collect_trajectories()
        update_advantates!(data)
        if device == gpu; data = map(dataâ‚œ -> device.(dataâ‚œ), data); end
        if actor_gpu.actor_model !== actor.actor_model; Flux.loadparams!(actor_gpu.actor_model, Flux.params(actor.actor_model)); end
        if critic_gpu !== critic; Flux.loadparams!(critic_gpu, Flux.params(critic)); end
        â„“Î¸, HÌ„, kl_div, num_iters = update_actor_multiple_iters!(actor_gpu, data, max_actor_iters)
        â„“Ï•, vÌ„ = update_critic!(critic_gpu, data)
        if actor_gpu.actor_model !== actor.actor_model; Flux.loadparams!(actor.actor_model, Flux.params(actor_gpu.actor_model)); end
        if critic_gpu !== critic; Flux.loadparams!(critic, Flux.params(critic_gpu)); end
        return â„“Î¸, â„“Ï•, HÌ„, kl_div, num_iters, vÌ„
    end

    episodes = length(returns)
    if steps % train_interval == 0
        for gradstep in 1:gradsteps
            â„“Î¸, â„“Ï•, H, kl, num_actor_iters, vÌ„ = train_actor_critic()
            stats[:â„“Î¸] = â„“Î¸
            stats[:â„“Ï•] = â„“Ï•
            stats[:H] = H
            stats[:kl] = kl
            stats[:vÌ„] = vÌ„
            stats[:num_actor_iters] = num_actor_iters
        end
    end
    
    @debug "learning stats" steps episodes stats...
    nothing
end