using MDPs
import MDPs: preexperiment, postepisode, poststep
using UnPack
using Random
using Flux
using Flux.Zygote
using Flux: Optimiser, ClipNorm

export PPOLearner

Base.@kwdef mutable struct PPOLearner <: AbstractHook
    envs::Vector{AbstractMDP}   # A vector of differently seeded environments.
    actor::Union{Actor, RecurrentActor}       # `Actor` maps states to action probabilities, and the input size of dimensionality of state space. `RecurrentActor` maps catenation of [latest action (onehot), latest reward, current state] to action probabilities, and therefore the input size is: number of actions + 1 + dimensionality of states
    critic                      # Any Flux model mapping ùëÜ -> ‚Ñù. If the actor is a `RecurrentActor`, the critic is also expected to be a recurrent model mapping ‚Ñù^|ùê¥| √ó ‚Ñù √ó ùëÜ -> ‚Ñù. i.e., like for the actor, the inputs are a catenation of [latest action (onehot), latest reward, current state].
    Œ≥::Float32 = 0.99           # discount factor. Used to calulate TD(Œª) advantages.
    nsteps::Int = 100           # numer of steps per iteration. So that total data per iteration = nenvs * nsteps. With longer nsteps, TD(Œª) returns are computed better.
    nepochs::Int = 10           # number of epochs per iteration.
    batch_size::Int = 256       # will be clipped to be between nenvs and nenvs * nsteps. Accordingly, nsteps per batch (which is truncated backprop horizon in case of RNNs) = batch_size / nenvs.
    entropy_bonus::Float32 = 0.01f0 # coefficient of the entropy term in the overall PPO loss, to encourage exploration.
    clipnorm::Float32 = Inf     # clip gradients by norm
    adam_weight_decay::Float32 = 0f0      # adam weight decay
    adam_epsilon::Float32 = 1f-7    # adam epsilon
    lr::Float32 = 0.0003        # adam learning rate
    Œª = 0.95f0                  # Used to calulate TD(Œª) advantages using Generalized Advantage Estimation (GAE) method.
    œµ = 0.2f0                   # epsilon used in PPO clip objective
    kl_target = 0.01            # In each iteration, early stop training if KL divergence from old policy exceeds this value.
    ppo = true                  # whether to use PPO clip objective. If false, standard advantange actor-critic (A2C) objective will be used.
    device = cpu                # `cpu` or `gpu`

    # data structures:
    optim = make_adam_optim(lr, (0.9, 0.999), adam_epsilon, clipnorm, adam_weight_decay)
    actor_gpu = device(deepcopy(actor))
    critic_gpu = device(deepcopy(critic))
    ùê°‚Çú = nothing                # hidden states of the RNNs between training iterations
    ùê¨‚Çú = nothing                # states of envs between training iterations
    ùêØ‚Çú = nothing                # values of the states of the envs between training iterations
    stats = Dict{Symbol, Any}()
end


function postepisode(ppo::PPOLearner; returns, steps, rng, kwargs...)
    @unpack envs, Œ≥, nsteps, nepochs, batch_size, entropy_bonus, Œª, œµ, kl_target, device = ppo

    isrecurrent = ppo.actor isa RecurrentActor
    m = size(state_space(envs[1]), 1)
    n = size(action_space(envs[1]), 1)
    N = length(envs)
    M = nsteps
    episodes = length(returns)

    if isnothing(ppo.ùê¨‚Çú)
        Threads.@threads for i in 1:N; reset!(envs[i], rng=rng); end
        ùê¨‚Çú = mapfoldl(state, hcat, envs) |> tof32
        if isrecurrent
            ùê¨‚Çú = vcat(zeros(Float32, n, N), zeros(Float32, 1, N), ùê¨‚Çú)
        end
        reset_rnn_state!.((ppo.actor, ppo.critic))
        init_ùê¨‚Çú = ùê¨‚Çú
        init_ùê°‚Çú = get_rnn_state.((ppo.actor, ppo.critic))
        init_ùêØ‚Çú = ppo.critic(ùê¨‚Çú)
    else
        init_ùê¨‚Çú = ppo.ùê¨‚Çú
        init_ùê°‚Çú = ppo.ùê°‚Çú
        init_ùêØ‚Çú = ppo.ùêØ‚Çú
    end

    function collect_trajectories(actor, critic)    
        ùê¨‚Çú = init_ùê¨‚Çú # start where we left off
        isrecurrent && set_rnn_state!.((actor, critic), init_ùê°‚Çú)
        ùêØ‚Çú = init_ùêØ‚Çú
        data = map(1:M) do t
            ùõë‚Çú, logùõë‚Çú = get_probs_logprobs(actor, ùê¨‚Çú)   # Forward actor
            ùêö‚Çú = fill(CartesianIndex(0, 0), 1, N)
            Threads.@threads for i in 1:N
                a·µ¢‚Çú = sample(rng, 1:n, ProbabilityWeights(ùõë‚Çú[:, i]))
                step!(envs[i], a·µ¢‚Çú; rng=rng)
                ùêö‚Çú[1, i] = CartesianIndex(a·µ¢‚Çú, i)
            end
            ùê´‚Çú = mapfoldl(reward, hcat, envs) |> tof32
            ùê¨‚Çú‚Ä≤= mapfoldl(state, hcat, envs) |> tof32
            ùêù‚Çú = mapfoldl(in_absorbing_state, hcat, envs) |> tof32
            ùê≠‚Çú = mapfoldl(truncated, hcat, envs) |> tof32
            if isrecurrent
                ùêö‚Çú_onehot = zeros(Float32, n, N)
                ùêö‚Çú_onehot[ùêö‚Çú] .= 1
                ùê¨‚Çú‚Ä≤ = vcat(ùêö‚Çú_onehot, ùê´‚Çú, ùê¨‚Çú‚Ä≤)
                ùê°‚Çú_backup = get_rnn_state(critic)  # create a backup
            end
            ùêØ‚Çú‚Ä≤ = critic(ùê¨‚Çú‚Ä≤)
            ùõÖ‚Çú = ùê´‚Çú + Œ≥ * (1f0 .- ùêù‚Çú) .* ùêØ‚Çú‚Ä≤ - ùêØ‚Çú
            data‚Çú = (ùê¨‚Çú, ùêö‚Çú, ùê´‚Çú, ùêù‚Çú, ùê≠‚Çú, ùê¨‚Çú‚Ä≤, ùõÖ‚Çú, ùõë‚Çú, logùõë‚Çú, ùêØ‚Çú)
            # ---------------- prepare for next step -------------------
            # set up states:
            ùê¨‚Çú = copy(ùê¨‚Çú‚Ä≤)
            any_reset = false
            for i in 1:N
                if ùêù‚Çú[1, i] + ùê≠‚Çú[1, i] > 0
                    reset!(envs[i]; rng=rng);
                    ùê¨‚Çú[:, i] .= 0f0
                    ùê¨‚Çú[end-m+1:end, i] .= tof32(state(envs[i]))
                    any_reset = true
                end
            end
            # setup rnn states:
            if isrecurrent && any_reset
                set_rnn_state!(critic, ùê°‚Çú_backup)
                reset_idxs::BitVector = ((ùêù‚Çú + ùê≠‚Çú) .> 0)[1, :]
                reset_rnn_state!.((actor, critic), (reset_idxs, reset_idxs));
            end
            ùêØ‚Çú = any_reset ? critic(ùê¨‚Çú) : ùêØ‚Çú‚Ä≤
            # ---------------------------------------------------------
            return data‚Çú
        end
        ppo.ùê¨‚Çú = ùê¨‚Çú
        ppo.ùêØ‚Çú = ùêØ‚Çú
        return data
    end
        
    # update advantages using GAE
    function update_advantates!(data)
        ùêÄ‚Çú‚Ä≤ = 0
        for data‚Çú in reverse(data)
            (ùê¨‚Çú, ùêö‚Çú, ùê´‚Çú, ùêù‚Çú, ùê≠‚Çú, ùê¨‚Çú‚Ä≤, ùõÖ‚Çú, ùõë‚Çú, logùõë‚Çú, ùêØ‚Çú) = data‚Çú
            ùõÖ‚Çú .+= Œ≥ * Œª * (1f0 .- ùêù‚Çú) .* ùêÄ‚Çú‚Ä≤
            ùêÄ‚Çú‚Ä≤ = ùõÖ‚Çú
        end
    end
        
    # update actor critic on entire data. Truncated backprop through time.
    function update_actor_critic_one_epoch!(actor, critic, data)
        ‚Ñì, vÃÑ, HÃÑ, kl = 0, 0, 0, 0
        isrecurrent && set_rnn_state!.((actor, critic), init_ùê°‚Çú)
        Œ∏ = Flux.params(actor, critic)
        batch_size = clamp(ppo.batch_size, N, N*M)
        batch_nsteps = batch_size √∑ N
        if !isrecurrent; data = shuffle(rng, data); end
        foreach(splitequal(M, batch_nsteps)) do timeindices
            ‚àá = gradient(Œ∏) do
                return mapfoldl(+, data[timeindices]) do (ùê¨‚Çú, ùêö‚Çú, ùê´‚Çú, ùêù‚Çú, ùê≠‚Çú, ùê¨‚Çú‚Ä≤, ùõÖ‚Çú, oldùõë‚Çú, oldlogùõë‚Çú, ùêØ‚Çú)
                    ùõë‚Çú, logùõë‚Çú = get_probs_logprobs(actor, ùê¨‚Çú)
                    if ppo.ppo
                        ùëü‚Çú =  ùõë‚Çú[ùêö‚Çú] ./ oldùõë‚Çú[ùêö‚Çú]
                        actor_loss‚Çú = -min.(ùëü‚Çú .* ùõÖ‚Çú, clamp.(ùëü‚Çú, 1-œµ, 1+œµ) .* ùõÖ‚Çú) |> mean
                    else
                        actor_loss‚Çú = -ùõÖ‚Çú .* logùõë‚Çú[ùêö‚Çú] |> mean
                    end
                    critic_loss‚Çú = Flux.mse(critic(ùê¨‚Çú), ùêØ‚Çú + ùõÖ‚Çú)
                    HÃÑ‚Çú = -sum(ùõë‚Çú .* logùõë‚Çú; dims=1) |> mean
                    loss‚Çú = actor_loss‚Çú + critic_loss‚Çú - entropy_bonus * HÃÑ‚Çú

                    if isrecurrent
                        reset_idxs::BitVector = Zygote.@ignore (cpu(ùêù‚Çú + ùê≠‚Çú) .> 0)[1, :]
                        reset_rnn_state!.((actor, critic), (reset_idxs, reset_idxs));
                    end

                    HÃÑ += Zygote.@ignore HÃÑ‚Çú / M
                    ‚Ñì += Zygote.@ignore loss‚Çú / M
                    vÃÑ += Zygote.@ignore mean(ùêØ‚Çú) / M
                    kl += Zygote.@ignore kldivergence(oldùõë‚Çú, ùõë‚Çú) / M

                    return loss‚Çú
                end
            end
            Flux.update!(ppo.optim, Œ∏, ‚àá)
        end
        return ‚Ñì, vÃÑ, HÃÑ, kl
    end

    function calc_kl_div(actor, data)
        kl = 0
        isrecurrent && set_rnn_state!(actor, init_ùê°‚Çú[1])
        return mapfoldl(+, data) do (ùê¨‚Çú, ùêö‚Çú, ùê´‚Çú, ùêù‚Çú, ùê≠‚Çú, ùê¨‚Çú‚Ä≤, ùõÖ‚Çú, oldùõë‚Çú, oldlogùõë‚Çú, ùêØ‚Çú)
            ùõë‚Çú, logùõë‚Çú = get_probs_logprobs(actor, ùê¨‚Çú)
            isrecurrent && reset_rnn_state!(actor, (cpu(ùêù‚Çú + ùê≠‚Çú) .> 0)[1, :]);
            return kldivergence(oldùõë‚Çú, ùõë‚Çú) / M
        end
        return ‚Ñì, vÃÑ, HÃÑ, kl
    end

    function update_actor_critic_with_early_stopping!(actor, critic, data, epochs)
        ‚Ñì, vÃÑ, HÃÑ, kl = 0, 0, 0, 0
        num_epochs = 0
        for epoch in 1:epochs
            ‚Ñì, vÃÑ, HÃÑ, kl = update_actor_critic_one_epoch!(actor, critic, data)
            num_epochs += 1
            kl = calc_kl_div(actor, data)
            kl >= kl_target && break
        end
        return ‚Ñì, vÃÑ, HÃÑ, kl, num_epochs
    end

    data = collect_trajectories(ppo.actor, ppo.critic)
    update_advantates!(data)
    if ppo.device == gpu; data = device(map(data‚Çú -> ppo.device.(data‚Çú), data)); end;

    Flux.loadparams!(ppo.actor_gpu, Flux.params(ppo.actor))
    Flux.loadparams!(ppo.critic_gpu, Flux.params(ppo.critic))

    ‚Ñì, vÃÑ, HÃÑ, kl, num_epochs = update_actor_critic_with_early_stopping!(ppo.actor_gpu, ppo.critic_gpu, data, nepochs)

    Flux.loadparams!(ppo.actor, Flux.params(ppo.actor_gpu))
    Flux.loadparams!(ppo.critic, Flux.params(ppo.critic_gpu))
    if isrecurrent
        set_rnn_state!.((ppo.actor, ppo.critic), get_rnn_state.((ppo.actor_gpu, ppo.critic_gpu)))
        ppo.ùê°‚Çú = get_rnn_state.((ppo.actor, ppo.critic))
    end

    ppo.stats[:‚Ñì] = ‚Ñì
    ppo.stats[:HÃÑ] = HÃÑ
    ppo.stats[:kl] = kl
    ppo.stats[:vÃÑ] = vÃÑ
    ppo.stats[:num_epochs] = num_epochs

    @debug "learning stats" steps episodes stats...
    nothing
end