using MDPs
import MDPs: preexperiment, postepisode, poststep
using UnPack
using Random
using Flux
using Flux.Zygote
using Flux: Optimiser, ClipNorm
import ProgressMeter: @showprogress, Progress, next!, finish!

export PPOLearner

Base.@kwdef mutable struct PPOLearner <: AbstractHook
    envs::Vector{AbstractMDP}   # A vector of differently seeded environments.
    actor::Union{Actor, RecurrentActor}       # `Actor` maps states to action probabilities, and the input size of dimensionality of state space. `RecurrentActor` maps catenation of [latest action (onehot), latest reward, current state] to action probabilities, and therefore the input size is: number of actions + 1 + dimensionality of states
    critic                      # Any Flux model mapping ùëÜ -> ‚Ñù. If the actor is a `RecurrentActor`, the critic is also expected to be a recurrent model mapping ‚Ñù^|ùê¥| √ó ‚Ñù √ó ùëÜ -> ‚Ñù. i.e., like for the actor, the inputs are a catenation of [latest action (onehot), latest reward, current state].
    Œ≥::Float32 = 0.99           # discount factor. Used to calulate TD(Œª) advantages.
    nsteps::Int = 100           # numer of steps per iteration. So that total data per iteration = nenvs * nsteps. With longer nsteps, TD(Œª) returns are computed better.
    nepochs::Int = 10           # number of epochs per iteration.
    minibatch_size::Int = 512
    entropy_bonus::Float32 = 0.01f0 # coefficient of the entropy term in the overall PPO loss, to encourage exploration.
    center_advantages::Bool = true # whether to center advantages to have zero mean.
    scale_advantages::Bool = true  # whether to scale advantages to have unit variance.
    clipnorm::Float32 = Inf     # clip gradients by norm
    adam_weight_decay::Float32 = 0f0      # adam weight decay
    adam_epsilon::Float32 = 1f-7    # adam epsilon
    lr::Float32 = 0.0003        # adam learning rate
    Œª = 0.95f0                  # Used to calulate TD(Œª) advantages using Generalized Advantage Estimation (GAE) method.
    œµ = 0.2f0                   # epsilon used in PPO clip objective
    kl_target = 0.01            # In each iteration, early stop training if KL divergence from old policy exceeds this value.
    ppo = true                  # whether to use PPO clip objective. If false, standard advantange actor-critic (A2C) objective will be used.
    device = cpu                # `cpu` or `gpu`
    progressmeter::Bool = false # Whether to show data and gradient updates progress using a progressmeter

    # data structures:
    optim = make_adam_optim(lr, (0.9, 0.999), adam_epsilon, clipnorm, adam_weight_decay)
    actor_gpu = device(deepcopy(actor))
    critic_gpu = device(deepcopy(critic))
    ùê°‚Çú = nothing                # hidden states of the RNNs between training iterations
    ùê¨‚Çú = nothing                # states of envs between training iterations
    stats = Dict{Symbol, Any}()
end


function postepisode(ppo::PPOLearner; returns, steps, rng, kwargs...)
    @unpack envs, Œ≥, nsteps, nepochs, minibatch_size, entropy_bonus, Œª, œµ, kl_target, device, progressmeter = ppo

    isrecurrent = ppo.actor isa RecurrentActor
    m = size(state_space(envs[1]), 1)
    n = size(action_space(envs[1]), 1)
    N = length(envs)
    M = nsteps
    B = N * M
    b = min(minibatch_size, B)
    episodes = length(returns)

    if isnothing(ppo.ùê¨‚Çú)
        Threads.@threads for i in 1:N; reset!(envs[i], rng=rng); end
        ùê¨‚Çú = mapfoldl(state, hcat, envs) |> tof32
        if isrecurrent
            ùê¨‚Çú = vcat(zeros(Float32, n, N), zeros(Float32, 1, N), ùê¨‚Çú)
        end
        init_ùê¨‚Çú = ùê¨‚Çú
        reset_rnn_state!.((ppo.actor, ppo.critic))
        init_ùê°‚Çú = get_rnn_state.((ppo.actor, ppo.critic))
    else
        init_ùê¨‚Çú = ppo.ùê¨‚Çú
        init_ùê°‚Çú = ppo.ùê°‚Çú
    end

    function collect_trajectories(actor, critic)    
        ùê¨‚Çú = init_ùê¨‚Çú # start where we left off
        isrecurrent && set_rnn_state!.((actor, critic), init_ùê°‚Çú)
        ùêØ‚Çú = critic(ùê¨‚Çú)
        data = []
        progress = Progress(M; color=:blue, desc="Collecting trajectories", enabled=progressmeter)
        for t in 1:M
            ùõë‚Çú, logùõë‚Çú = get_probs_logprobs(actor, ùê¨‚Çú)   # Forward actor
            ùêö‚Çú = zeros(Int, 1, N)
            Threads.@threads for i in 1:N
                a·µ¢‚Çú = sample(rng, 1:n, ProbabilityWeights(ùõë‚Çú[:, i]))
                step!(envs[i], a·µ¢‚Çú; rng=rng)
                ùêö‚Çú[1, i] = a·µ¢‚Çú
            end
            ùê´‚Çú = mapfoldl(reward, hcat, envs) |> tof32
            ùê¨‚Çú‚Ä≤= mapfoldl(state, hcat, envs) |> tof32
            ùêù‚Çú = mapfoldl(in_absorbing_state, hcat, envs) |> tof32
            ùê≠‚Çú = mapfoldl(truncated, hcat, envs) |> tof32
            if isrecurrent
                ùêö‚Çú_onehot = @views Flux.OneHotArrays.onehotbatch(ùêö‚Çú[1, :], 1:n) |> tof32
                ùê¨‚Çú‚Ä≤ = vcat(ùêö‚Çú_onehot, ùê´‚Çú, ùê¨‚Çú‚Ä≤)
                ùê°‚Çú_backup = get_rnn_state(critic)  # create a backup
            end
            ùêØ‚Çú‚Ä≤ = critic(ùê¨‚Çú‚Ä≤)
            ùõÖ‚Çú = ùê´‚Çú + Œ≥ * (1f0 .- ùêù‚Çú) .* ùêØ‚Çú‚Ä≤ - ùêØ‚Çú
            data‚Çú = (ùê¨‚Çú, ùêö‚Çú, ùê´‚Çú, ùêù‚Çú, ùê≠‚Çú, ùê¨‚Çú‚Ä≤, ùõÖ‚Çú, ùõë‚Çú, logùõë‚Çú, ùêØ‚Çú)
            push!(data, data‚Çú)
            # ---------------- prepare for next step -------------------
            # set up states:
            ùê¨‚Çú = copy(ùê¨‚Çú‚Ä≤)
            any_reset = false
            for i in 1:N
                if ùêù‚Çú[1, i] + ùê≠‚Çú[1, i] > 0
                    reset!(envs[i]; rng=rng);
                    ùê¨‚Çú[:, i] .= 0f0
                    ùê¨‚Çú[end-m+1:end, i] .= tof32(state(envs[i]))
                    any_reset = true
                end
            end
            # setup rnn states:
            if isrecurrent && any_reset
                set_rnn_state!(critic, ùê°‚Çú_backup)
                reset_idxs::BitVector = ((ùêù‚Çú + ùê≠‚Çú) .> 0)[1, :]
                reset_rnn_state!.((actor, critic), (reset_idxs, reset_idxs));
            end
            ùêØ‚Çú = any_reset ? critic(ùê¨‚Çú) : ùêØ‚Çú‚Ä≤
            next!(progress)
        end
        ppo.ùê¨‚Çú = ùê¨‚Çú
        finish!(progress)
        return data
    end

    # update advantages using GAE
    function update_advantates!(data)
        ùêÄ‚Çú‚Ä≤ = 0
        for data‚Çú in reverse(data)
            (ùê¨‚Çú, ùêö‚Çú, ùê´‚Çú, ùêù‚Çú, ùê≠‚Çú, ùê¨‚Çú‚Ä≤, ùõÖ‚Çú, ùõë‚Çú, logùõë‚Çú, ùêØ‚Çú) = data‚Çú
            ùõÖ‚Çú .+= Œ≥ * Œª * (1f0 .- ùêù‚Çú) .* ùêÄ‚Çú‚Ä≤
            ùêÄ‚Çú‚Ä≤ = ùõÖ‚Çú
        end
    end

    function flattened(data)
        ùê¨, ùêö, ùê´, ùêù, ùê≠, ùê¨‚Ä≤, ùõÖ, ùõë, logùõë, ùêØ = map(i -> mapfoldl(d -> d[i], hcat, data), 1:length(data[1]))
        return ùê¨, ùêö, ùê´, ùêù, ùê≠, ùê¨‚Ä≤, ùõÖ, ùõë, logùõë, ùêØ
    end


    function to_gpu(data)
        ùê¨, ùêö, ùê´, ùêù, ùê≠, ùê¨‚Ä≤, ùõÖ, ùõë, logùõë, ùêØ = data
        ùê¨, ùê´, ùê¨‚Ä≤, ùõÖ, ùõë, logùõë, ùêØ = gpu.((ùê¨, ùê´, ùê¨‚Ä≤, ùõÖ, ùõë, logùõë, ùêØ))
        return ùê¨, ùêö, ùê´, ùêù, ùê≠, ùê¨‚Ä≤, ùõÖ, ùõë, logùõë, ùêØ
    end

    function ppo_loss(actor, critic, ùê¨, ùêö, ùõÖ, oldùõë, ùêØ)
        # ---- critic loss ----
        critic_loss = Flux.mse(critic(ùê¨), ùêØ + ùõÖ)
        # ---- actor loss ----
        ùêö = Zygote.@ignore CartesianIndex.(zip(ùêö, (1:size(ùêØ, 2))'))
        ùõë, logùõë = get_probs_logprobs(actor, ùê¨)
        ùõÖ = !ppo.center_advantages ? ùõÖ : Zygote.@ignore ùõÖ .- mean(ùõÖ) 
        ùõÖ = !ppo.scale_advantages ? ùõÖ : Zygote.@ignore ùõÖ ./ (std(ùõÖ) + 1e-8)
        if ppo.ppo
            ùëü =  ùõë[ùêö] ./ oldùõë[ùêö]
            actor_loss = -min.(ùëü .* ùõÖ, clamp.(ùëü, 1-œµ, 1+œµ) .* ùõÖ) |> mean
        else
            actor_loss = -ùõÖ .* logùõë[ùêö] |> mean
        end
        # ---- entropy bonus ----
        entropy = -sum(ùõë .* logùõë; dims=1) |> mean
        # ---- total loss ----
        return actor_loss + critic_loss - entropy_bonus * entropy
    end

    function update_actor_critic_one_epoch_recurrent!(actor, critic, data)
        loss, Œ∏ = 0f0, Flux.params(actor, critic)
        ùê¨, ùêö, _, ùêù, ùê≠, _, ùõÖ, oldùõë, _, ùêØ = data
        ùê¨, ùêö, ùêù, ùê≠, ùõÖ, oldùõë, ùêØ = map(ùê± -> reshape(ùê±, :, N, M), (ùê¨, ùêö, ùêù, ùê≠, ùõÖ, oldùõë, ùêØ))    # reshape to 3D to make time as last axis.
        nsgdsteps = ceil(Int, B / b)
        _N = ceil(Int, N / nsgdsteps) # num envs per minibatch
        progress = Progress(N; desc="Performing gradient updates", color=:blue, enabled=progressmeter)
        for env_indices in splitequal(N, _N)
            _ùê¨, _ùêö, _ùêù, _ùê≠, _ùõÖ, _oldùõë, _ùêØ = map(ùê± -> ùê±[:, env_indices, :], (ùê¨, ùêö, ùêù, ùê≠, ùõÖ, oldùõë, ùêØ))
            set_rnn_state!.((actor, critic), init_ùê°‚Çú, (env_indices, env_indices))
            _loss, _‚àá =  withgradient(Œ∏) do
                return mapfoldl(+, 1:M) do t
                    _ùê¨‚Çú, _ùêö‚Çú, _ùêù‚Çú, _ùê≠‚Çú, _ùõÖ‚Çú, _oldùõë‚Çú, _ùêØ‚Çú = Zygote.@ignore @views map(ùê± -> ùê±[:, :, t], (_ùê¨, _ùêö, _ùêù, _ùê≠, _ùõÖ, _oldùõë, _ùêØ))
                    _loss‚Çú = ppo_loss(actor, critic, _ùê¨‚Çú, _ùêö‚Çú, _ùõÖ‚Çú, _oldùõë‚Çú, _ùêØ‚Çú)
                    if isrecurrent
                        reset_idxs::BitVector = Zygote.@ignore ((_ùêù‚Çú + _ùê≠‚Çú) .> 0)[1, :]
                        reset_rnn_state!.((actor, critic), (reset_idxs, reset_idxs));
                    end
                    return _loss‚Çú / M
                end
            end
            Flux.update!(ppo.optim, Œ∏, _‚àá)
            loss += _loss * length(env_indices) / N
            next!(progress; step=length(env_indices))
        end
        finish!(progress)
        return loss
    end

    function update_actor_critic_one_epoch!(actor, critic, data)
        loss, Œ∏ = 0f0, Flux.params(actor, critic)
        ùê¨, ùêö, _, _, _, _, ùõÖ, oldùõë, _, ùêØ = data
        nsgdsteps = ceil(Int, B / b)
        progress = Progress(nsgdsteps; desc="Performing gradient updates", color=:blue, enabled=progressmeter)
        for i in 1:nsgdsteps
            mb_indices = rand(rng, 1:B, b)
            _ùê¨, _ùêö, _ùõÖ, _oldùõë, _ùêØ = map(ùê± -> ùê±[:, mb_indices], (ùê¨, ùêö, ùõÖ, oldùõë, ùêØ))
            _loss, _‚àá = withgradient(Œ∏) do
                return ppo_loss(actor, critic, _ùê¨, _ùêö, _ùõÖ, _oldùõë, _ùêØ)
            end
            Flux.update!(ppo.optim, Œ∏, _‚àá)
            loss += _loss / nsgdsteps
            next!(progress)
        end
        finish!(progress)
        return loss
    end

    # function update_actor_critic_one_epoch_recurrent_truncated_bptt!(actor, critic, data)
    #     loss = 0
    #     ùê¨, ùêö, _, ùêù, ùê≠, _, ùõÖ, oldùõë, _, ùêØ = data
    #     ùê¨, ùêö, ùêù, ùê≠, ùõÖ, oldùõë, ùêØ = @views map(ùê± -> reshape(ùê±, :, N, M), (ùê¨, ùêö, ùêù, ùê≠, ùõÖ, oldùõë, ùêØ))    # reshape to 3D to make time as last axis.
    #     set_rnn_state!.((actor, critic), init_ùê°‚Çú)
    #     Œ∏ = Flux.params(actor, critic)
    #     _M = ceil(Int, b/N)
    #     foreach(splitequal(M, _M)) do timeindices
    #         _loss, ‚àá = withgradient(Œ∏) do
    #             return mapfoldl(+, timeindices) do t
    #                 (ùê¨‚Çú, ùêö‚Çú, ùêù‚Çú, ùê≠‚Çú, ùõÖ‚Çú, oldùõë‚Çú, ùêØ‚Çú) = Zygote.@ignore @views map(ùê± -> ùê±[:, :, t], (ùê¨, ùêö, ùêù, ùê≠, ùõÖ, oldùõë, ùêØ))
    #                 loss‚Çú = ppo_loss(actor, critic, ùê¨‚Çú, ùêö‚Çú, ùõÖ‚Çú, oldùõë‚Çú, ùêØ‚Çú)
    #                 if isrecurrent
    #                     reset_idxs::BitVector = Zygote.@ignore (cpu(ùêù‚Çú + ùê≠‚Çú) .> 0)[1, :]
    #                     reset_rnn_state!.((actor, critic), (reset_idxs, reset_idxs));
    #                 end
    #                 return loss‚Çú / M
    #             end
    #         end
    #         loss += _loss
    #         Flux.update!(ppo.optim, Œ∏, ‚àá)
    #     end
    #     return loss
    # end

    function calculate_stats(actor, critic, data)
        HÃÑ, vÃÑ, kl = 0, 0, 0
        ùê¨, _, _, ùêù, ùê≠, _, _, oldùõë, _, _ = data
        ùê¨, ùêù, ùê≠, oldùõë = map(ùê± -> reshape(ùê±, :, N, M), (ùê¨, ùêù, ùê≠, oldùõë))
        isrecurrent && set_rnn_state!.((actor, critic), init_ùê°‚Çú)
        for t in 1:M
            ùê¨‚Çú, ùêù‚Çú, ùê≠‚Çú, oldùõë‚Çú = map(ùê± -> ùê±[:, :, t], (ùê¨, ùêù, ùê≠, oldùõë))
            ùõë‚Çú, logùõë‚Çú = get_probs_logprobs(actor, ùê¨‚Çú)
            HÃÑ += -sum(ùõë‚Çú .* logùõë‚Çú; dims=1) |> mean
            kl += kldivergence(oldùõë‚Çú, ùõë‚Çú)
            vÃÑ += critic(ùê¨‚Çú) |> mean
            if isrecurrent
                reset_idxs::BitVector = ((ùêù‚Çú + ùê≠‚Çú) .> 0)[1, :]
                reset_rnn_state!.((actor, critic), (reset_idxs, reset_idxs));
            end
        end
        return (kl, HÃÑ, vÃÑ) ./ M
    end

    function update_actor_critic_with_early_stopping!(actor, critic, data, epochs)
        ‚Ñì, kl, HÃÑ, vÃÑ = 0, 0, 0, 0
        num_epochs = 0
        update_fn = isrecurrent ? update_actor_critic_one_epoch_recurrent! : update_actor_critic_one_epoch!
        for epoch in 1:epochs
            ‚Ñì = update_fn(actor, critic, data)
            num_epochs += 1
            kl, HÃÑ, vÃÑ = calculate_stats(actor, critic, data)
            kl >= kl_target && break
        end
        return ‚Ñì, kl, HÃÑ, vÃÑ, num_epochs
    end

    data = collect_trajectories(ppo.actor, ppo.critic)
    update_advantates!(data)
    data = flattened(data)

    if ppo.device == gpu; data = to_gpu(data); end;

    Flux.loadparams!(ppo.actor_gpu, Flux.params(ppo.actor))
    Flux.loadparams!(ppo.critic_gpu, Flux.params(ppo.critic))

    ‚Ñì, kl, HÃÑ, vÃÑ, num_epochs = update_actor_critic_with_early_stopping!(ppo.actor_gpu, ppo.critic_gpu, data, nepochs)
    
    Flux.loadparams!(ppo.actor, Flux.params(ppo.actor_gpu))
    Flux.loadparams!(ppo.critic, Flux.params(ppo.critic_gpu))
    if isrecurrent
        set_rnn_state!.((ppo.actor, ppo.critic), get_rnn_state.((ppo.actor_gpu, ppo.critic_gpu)))
        ppo.ùê°‚Çú = get_rnn_state.((ppo.actor, ppo.critic))
    end

    ppo.stats[:‚Ñì] = ‚Ñì
    ppo.stats[:HÃÑ] = HÃÑ
    ppo.stats[:kl] = kl
    ppo.stats[:vÃÑ] = vÃÑ
    ppo.stats[:num_epochs] = num_epochs

    @debug "learning stats" steps episodes stats...
    nothing
end