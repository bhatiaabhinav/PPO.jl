using MDPs
import MDPs: preexperiment, postepisode, poststep
using Base.Iterators: partition, product
using Random
using Flux
using Flux.Zygote
import ProgressMeter: @showprogress, Progress, next!, finish!

export PPOLearner

"""
    PPOLearner(; envs, actor, critic, Î³=0.99, nsteps=2048, nepochs=10, batch_size=64, entropy_bonus=0.0, decay_ent_bonus=false, normalize_advantages=true, clipnorm=0.5, adam_weight_decay=0.0, adam_epsilon=1e-7, lr_actor=0.0003, lr_critic=0.0003, decay_lr=false, min_lr=1.25e-5, Î»=0.95, Ïµ=0.2, kl_target=Inf, ppo=true, early_stop_critic=false, device=cpu, progressmeter=false, iters_per_postepisode::Int=1)

A hook that performs an iteration of Proximal Policy Optimization (PPO) in `postepisode` callback. Default hyperparameters are similar to those in Stable Baselines3 PPO implementation (https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html).

# Arguments
- `envs::AbstractVecEnv`: A collection of environments to collect data from. Should be of type `AbstractVecEnv`. See `VecEnv` in MDPs.jl to convert an array of environments to a single vectorized environment.
- `actor`: A PPO policy to optimize. Either PPOActorDiscrete or PPOActorContinuous.
- `critic`: A Flux model with recurrence type same as actor.
- `Î³::Float32=0.99`: Discount factor. Used to calulate TD(Î») advantages.
- `nsteps::Int=2048`: Numer of steps per iteration. So that total data per iteration = nenvs * nsteps. With longer nsteps, TD(Î») returns are computed better.
- `nepochs::Int=10`: Number of epochs per iteration.
- `batch_size::Int=64`: Minibatch size. Should be a multiple of `nsteps` when using recurrent or transformer actor.
- `entropy_bonus::Float32=0.0`: Coefficient of the entropy term in the overall PPO loss, to encourage exploration.
- `entropy_method::Symbol=:regularized`: `:regularized` or `:maximized`. If `:regularized`, entropy bonus is added to the loss. If `:maximized`, entropy bonus is added to the reward at each timestep (as in SAC).
- `decay_ent_bonus::Bool=false`: Whether to decay entropy bonus over time to 0, by the end of training (after `max_trials` iterations).
- `normalize_advantages::Bool=true`: Whether to center and scale advantages to have zero mean and unit variance
- `clipnorm::Float32=0.5`: Clip gradients by global norm
- `adam_weight_decay::Float32=0.0`: Adam weight decay
- `adam_epsilon::Float32=1e-7`: Adam epsilon
- `lr_actor::Float32=0.0003`: Adam learning rate for actor
- `lr_critic::Float32=0.0003`: Adam learning rate for critic
- `decay_lr::Bool=false`: Whether to decay learning rate over time to 0, by the end of training (after `max_trials` iterations).
- `min_lr::Float32=1.25e-5`: Minimum learning rate to decay to
- `Î»::Float32=0.95`: Used to calulate TD(Î») advantages using Generalized Advantage Estimation (GAE) method.
- `Ïµ::Float32=0.2`: Epsilon used in PPO clip objective
- `kl_target=Inf`: In each iteration, early stop actor training if KL divergence from old policy exceeds this value. Disbled if `kl_target=Inf` (default). A commonly used value, when enabled, is 0.01.
- `ppo=true`: Whether to use PPO clip objective. If false, standard advantange actor-critic (A2C) objective will be used.
- `early_stop_critic=false`: Whether to early stop training critic (along with actor) if KL divergence from old policy exceeds `kl_target`.
- `device=cpu`: `cpu` or `gpu`
- `progressmeter=false`: Whether to show data and gradient updates progress using a progressmeter (useful for debugging).
- `iters_per_postepisode::Int=1`: Number of training iterations per postepisode callback. Useful for debugging.
"""
Base.@kwdef mutable struct PPOLearner <: AbstractHook
    envs::AbstractVecEnv        # a vectorized environment
    actor::PPOActor
    critic                      # some model with recurrence type same as actor
    Î³::Float32 = 0.99           # discount factor. Used to calulate TD(Î») advantages.
    nsteps::Int = 2048           # numer of steps per iteration. So that total data per iteration = nenvs * nsteps. With longer nsteps, TD(Î») returns are computed better.
    nepochs::Int = 10           # number of epochs per iteration.
    batch_size::Int = 64        # minibatch size
    entropy_bonus::Float32 = 0.0f0 # coefficient of the entropy term in the overall PPO loss, to encourage exploration.
    entropy_method::Symbol = :regularized  # :regularized, :maximized
    decay_ent_bonus::Bool = false # whether to decay entropy bonus
    normalize_advantages::Bool = true # whether to center and scale advantages to have zero mean and unit variance
    clipnorm::Float32 = 0.5     # clip gradients by global norm
    adam_weight_decay::Float32 = 0f0      # adam weight decay
    adam_epsilon::Float32 = 1f-7    # adam epsilon
    lr_actor::Float32 = 0.0003        # adam learning rate for actor
    lr_critic::Float32 = 0.0003        # adam learning rate for critic
    decay_lr::Bool = false      # whether to decay learning rate
    min_lr::Float32 = 1.25e-5   # minimum learning rate to decay to
    Î»::Float32 = 0.95f0                  # Used to calulate TD(Î») advantages using Generalized Advantage Estimation (GAE) method.
    Ïµ::Float32 = 0.2f0                   # epsilon used in PPO clip objective
    kl_target = Inf            # In each iteration, early stop training if KL divergence from old policy exceeds this value.
    ppo = true                  # whether to use PPO clip objective. If false, standard advantange actor-critic (A2C) objective will be used.
    early_stop_critic = false
    device = cpu                # `cpu` or `gpu`
    progressmeter::Bool = false # Whether to show data and gradient updates progress using a progressmeter
    iters_per_postepisode::Int = 1  # number of training iterations per postepisode callback

    # data structures:
    optim_actor = make_adam_optim(lr_actor, (0.9, 0.999), adam_epsilon, 0)
    optim_critic = make_adam_optim(lr_critic, (0.9, 0.999), adam_epsilon, adam_weight_decay)  # regularize critic with weight decay (l2 norm) but don't regularize actor
    actor_gpu = device(deepcopy(actor))
    critic_gpu = device(deepcopy(critic))

    stats = Dict{Symbol, Any}()
end

function preexperiment(ppo::PPOLearner; rng, kwargs...)
    reset!(ppo.envs; rng=rng)
    ppo.stats[:iterations] = 0
end

function postepisode(ppo::PPOLearner; returns, max_trials, rng, kwargs...)
    M, N = ppo.nsteps, length(ppo.envs)
    experiment_progress = length(returns) / max_trials
    if ppo.decay_lr
        actor_lr, critic_lr = (ppo.lr_actor, ppo.lr_critic) .* (1 - experiment_progress)
        actor_lr, critic_lr = max(actor_lr, ppo.min_lr), max(critic_lr, ppo.min_lr)
        ppo.optim_actor[end].eta, ppo.optim_critic[end].eta = actor_lr, critic_lr
        ppo.stats[:lr_actor], ppo.stats[:lr_critic] = actor_lr, critic_lr
    end
    entropy_bonus = ppo.decay_ent_bonus ? ppo.entropy_bonus * (1 - experiment_progress) : ppo.entropy_bonus
    if ppo.actor.recurtype != MARKOV && ppo.batch_size % M != 0
        new_batch_size = clamp(round(Int, ppo.batch_size / M) * M, M, M * N)
        @warn "batch_size ($(ppo.batch_size)) is not a multiple of nsteps ($M). Changing batch_size to $new_batch_size"
        ppo.batch_size = new_batch_size
    end

    function do_full_ppo_iteration()
        ğ¬, ğš, ğ›‘, logğ›‘, ğ«, ğ­, ğ = collect_trajectories(ppo, ppo.actor_gpu, entropy_bonus, ppo.device, rng) |> ppo.device
        if eltype(ğš) <: Integer; ğš = cpu(ğš); end
        Flux.reset!(ppo.critic_gpu)
        ğ¯, ğ›… = get_values_advantages(ppo, ppo.critic_gpu, ğ¬, ğ«, ğ­, ğ, ppo.Î³, ppo.Î»)

        stop_actor_training, kl = false, 0f0
        losses, actor_losses, critic_losses = [], [], []
        Î¸ = Flux.params(ppo.actor_gpu, ppo.critic_gpu)
        while length(losses) < ppo.nepochs
            desc = stop_actor_training ? "Train Critic Epoch $(length(losses)+1)" : "Train Actor-Critic Epoch $(length(losses)+1)"
            progress = Progress(M * N; desc=desc, color=:magenta, enabled=ppo.progressmeter)
            loss, actor_loss, critic_loss = 0, 0, 0
            data_indices = cartesian_product(M, N)
            data_indices = ppo.actor.recurtype == MARKOV ? shuffle(rng, data_indices) : data_indices
            for mb_indices in partition(data_indices, ppo.batch_size)
                mb_ğ¬, mb_ğš, mb_ğ¯, mb_ğ›…, mb_ğ›‘, mb_logğ›‘ = map(ğ± -> ğ±[:, mb_indices], (ğ¬, ğš, ğ¯, ğ›…, ğ›‘, logğ›‘))
                if ppo.actor.recurtype != MARKOV 
                    mb_ğ¬, mb_ğš, mb_ğ¯, mb_ğ›…, mb_ğ›‘, mb_logğ›‘ = map(ğ± -> reshape(ğ±, :, M, length(mb_indices) Ã· M), (mb_ğ¬, mb_ğš, mb_ğ¯, mb_ğ›…, mb_ğ›‘, mb_logğ›‘))
                end
                Flux.reset!(ppo.actor_gpu)
                Flux.reset!(ppo.critic_gpu)
                âˆ‡ = gradient(Î¸) do
                    mb_loss, mb_actor_loss, mb_critic_loss = ppo_loss(ppo, ppo.actor_gpu, ppo.critic_gpu, mb_ğ¬, mb_ğš, mb_ğ¯, mb_ğ›…, mb_ğ›‘, mb_logğ›‘, Float32(!stop_actor_training), 0.5f0, entropy_bonus)
                    actor_loss += mb_actor_loss * length(mb_indices) / (M * N)
                    critic_loss += mb_critic_loss * length(mb_indices) / (M * N)
                    loss += mb_loss * length(mb_indices) / (M * N)
                    return mb_loss
                end
                ppo.clipnorm < Inf && clip_global_norm!(âˆ‡, Î¸, ppo.clipnorm)
                !stop_actor_training && Flux.update!(ppo.optim_actor, Flux.params(ppo.actor_gpu), âˆ‡)
                Flux.update!(ppo.optim_critic, Flux.params(ppo.critic_gpu), âˆ‡)
                next!(progress; step=length(mb_indices))
            end
            finish!(progress)
            push!(losses, loss)
            push!(critic_losses, critic_loss)
            if !stop_actor_training
                push!(actor_losses, actor_loss)
                Flux.reset!(ppo.actor_gpu)
                kl = get_kl_div(ppo.actor_gpu, ğ¬, ğš, ğ›‘, logğ›‘)
                stop_actor_training = kl >= ppo.kl_target
            end
            stop_actor_training && ppo.early_stop_critic && break
        end
        Flux.reset!(ppo.actor_gpu)
        Flux.reset!(ppo.critic_gpu)
        HÌ„, vÌ„ = mean(get_entropy(ppo.actor_gpu, ğ¬)), mean(get_values(ppo.critic_gpu, ğ¬, ppo.actor.recurtype))

        Flux.loadparams!(ppo.actor, Flux.params(ppo.actor_gpu))
        Flux.loadparams!(ppo.critic, Flux.params(ppo.critic_gpu))

        ppo.stats[:â„“] = losses[end]
        ppo.stats[:actor_loss] = actor_losses[end]
        ppo.stats[:critic_loss] = critic_losses[end]
        ppo.stats[:HÌ„] = HÌ„
        ppo.stats[:iteration_kl] = kl
        ppo.stats[:vÌ„] = vÌ„
        ppo.stats[:iteration_actor_epochs] = length(actor_losses)
        ppo.stats[:iteration_critic_epochs] = length(critic_losses)
        ppo.stats[:iterations] += 1
        ppo.stats[:ent_bonus] = entropy_bonus
        ppo.stats[:iteration_RÌ„] = mean(sum(ğ«, dims=2))
        ppo.stats[:iteration_rÌ„] = ppo.stats[:iteration_RÌ„] / M
        if ppo.actor isa PPOActorContinuous
            ppo.stats[:logstd] = string(ppo.actor.logstd)
        end
    end

    for iter_num in 1:ppo.iters_per_postepisode
        do_full_ppo_iteration()
    end

    nothing
end


function collect_trajectories(ppo::PPOLearner, actor, ent_coeff, device, rng)
    state_dim = size(state_space(ppo.envs), 1)
    isdiscrete = action_space(ppo.envs) isa IntegerSpace
    if isdiscrete
        nactions = length(action_space(ppo.envs))
    else
        action_dim = size(action_space(ppo.envs), 1)
    end
    M, N = ppo.nsteps, length(ppo.envs)

    if state_dim > 50
        ğ¬ = zeros(Float32, state_dim, M, N) |> device
    else
        ğ¬ = zeros(Float32, state_dim, M, N)
    end
    if isdiscrete
        ğš = zeros(Int, 1, M, N)
        ğ›‘ = zeros(Float32, nactions, M, N)
        logğ›‘ = zeros(Float32, nactions, M, N)
    else
        ğš = zeros(Float32, action_dim, M, N)
        ğ›‘ = zeros(Float32, 1, M, N)
        logğ›‘ = zeros(Float32, 1, M, N)
    end
    ğ« = zeros(Float32, 1, M, N)
    ğ­ = zeros(Float32, 1, M, N)
    ğ = zeros(Float32, 1, M, N)

    progress = Progress(M; color=:white, desc="(Iter $(ppo.stats[:iterations] + 1)) Collecting trajectories", enabled=ppo.progressmeter)

    Flux.reset!(actor)
    for t in 1:M
        reset!(ppo.envs, false; rng=rng)
        ğ¬â‚œ = state(ppo.envs) |> tof32
        if state_dim > 50
            ğ¬[:, t, :] .= device(ğ¬â‚œ)
        else
            ğ¬[:, t, :] .= ğ¬â‚œ
        end
        if isdiscrete
            @assert actor isa PPOActorDiscrete
            if ppo.actor.recurtype âˆˆ (MARKOV, RECURRENT)
                ğ›‘â‚œ, logğ›‘â‚œ = get_probs_logprobs(actor, device(ğ¬â‚œ)) |> cpu
            elseif ppo.actor.recurtype == TRANSFORMER
                s_t = ğ¬[:, 1:t, :]
                if state_dim > 50
                    s_gpu = s_t
                else
                    s_gpu = s_t |> device
                end
                ğ›‘â‚œ, logğ›‘â‚œ = get_probs_logprobs(actor, s_gpu) |> cpu
                ğ›‘â‚œ, logğ›‘â‚œ = ğ›‘â‚œ[:, t, :], logğ›‘â‚œ[:, t, :]
                s_gpu = nothing
                s_t = nothing
            end
            ğšâ‚œ = reshape([sample(rng, 1:nactions, ProbabilityWeights(ğ›‘â‚œ[:, i])) for i in 1:N], 1, N)
            ğš[:, t, :] = ğšâ‚œ
            ğ›‘[:, t, :] = ğ›‘â‚œ
            logğ›‘[:, t, :] = logğ›‘â‚œ
            ents = get_entropy(actor, ğ›‘â‚œ, logğ›‘â‚œ)
        else
            @assert actor isa PPOActorContinuous
            if ppo.actor.recurtype âˆˆ (MARKOV, RECURRENT)
                ğšâ‚œ, logğ›‘â‚œ, logğ›”â‚œ = sample_action_logprobs(actor, rng, device(ğ¬â‚œ); return_logstd=true) |> cpu
            else
                ğšâ‚œ, logğ›‘â‚œ, logğ›”â‚œ = sample_action_logprobs(actor, rng, device(ğ¬[:, 1:t, :]); return_logstd=true) |> cpu
                ğšâ‚œ, logğ›‘â‚œ = ğšâ‚œ[:, end, :], logğ›‘â‚œ[:, end, :]
            end
            ğš[:, t, :] = ğšâ‚œ
            logğ›‘[:, t, :] = logğ›‘â‚œ
            ğ›‘ = exp.(logğ›‘)
            ents = get_gaussian_entropy(logğ›”â‚œ)
        end

        if isdiscrete
            _ğšâ‚œ = ğšâ‚œ[1, :]
        else
            Tâ‚ = ppo.envs |> action_space |> eltype |> eltype
            _ğšâ‚œ = convert(Matrix{Tâ‚}, ğšâ‚œ) |> eachcol .|> copy    # eachcol makes it a vector of vectors
            _ğšâ‚œ = convert(Matrix{Tâ‚}, ğšâ‚œ)
        end
        step!(ppo.envs, _ğšâ‚œ; rng=rng)
        ğ«â‚œ = reward(ppo.envs)' |> tof32
        if ppo.entropy_method == :maximized && ent_coeff > 0
            @assert size(ents) == size(ğ«â‚œ)  "size mismatch: $(size(ents)) != $(size(ğ«â‚œ))"
            ğ«â‚œ += ent_coeff * ents
        end
        ğ«[:, t, :] = ğ«â‚œ
        ğ­â‚œ = in_absorbing_state(ppo.envs)' |> tof32
        ğ­[:, t, :] = ğ­â‚œ
        ğâ‚œ = (in_absorbing_state(ppo.envs) .|| truncated(ppo.envs))' |> tof32
        ğ[:, t, :] = ğâ‚œ

        next!(progress)
    end
    finish!(progress)
    return ğ¬, ğš, ğ›‘, logğ›‘, ğ«, ğ­, ğ
end


function get_values_advantages(ppo::PPOLearner, critic, ğ¬, ğ«, ğ­, ğ, Î³, Î»)
    ğ¬â‚œâ€² = unsqueeze(state(ppo.envs), dims=2) |> tof32
    ğ¬â‚œâ€² = convert(typeof(ğ¬), ğ¬â‚œâ€²)
    _ğ¯ = get_values(critic, hcat(ğ¬, ğ¬â‚œâ€²), ppo.actor.recurtype)
    ğ¯, ğ¯â€² = _ğ¯[:, 1:end-1, :], _ğ¯[:, 2:end, :]
    ğ›… = ğ« + Î³ * (1 .- ğ­) .* ğ¯â€² - ğ¯
    should_ignore_error = ğ - ğ­  # if a trajectory was truncated, set Î´ = 0. This state should not contribute to the loss.
    should_ignore_error[1, end, :] .= 0 # since we handled the last state of each trajectory properly
    ğ›… .*= (1f0 .- should_ignore_error)
    ğ€â‚œâ€² = 0
    for t in reverse(1:size(ğ›…, 2))
        ğ›…[:, t:t, :] .+= (1f0 .- ğ[:, t:t, :]) .* Î³ .* Î» .* ğ€â‚œâ€²
        ğ€â‚œâ€² = ğ›…[:, t:t, :]
    end
    return ğ¯, ğ›…
end


function ppo_loss(ppo::PPOLearner, actor, critic, ğ¬, ğš, ğ¯, ğ›…, oldğ›‘, oldlogğ›‘, actor_coeff, critic_coeff, ent_coeff)
    critic_loss = critic_coeff > 0 ? get_critic_loss(critic, ğ¬, ğ¯, ğ›…, ppo.actor.recurtype) : 0f0
    ğ›… = !ppo.normalize_advantages ? ğ›… : Zygote.@ignore (ğ›… .- mean(ğ›…)) ./ (std(ğ›…) + 1e-8)
    actor_loss, entropy = actor_coeff > 0 ? get_loss_and_entropy(actor, ğ¬, ğš, ğ›…, oldğ›‘, oldlogğ›‘, ppo.Ïµ, ppo.ppo) : (0f0, 0f0)
    total_loss = actor_loss + critic_coeff * critic_loss
    if ppo.entropy_method == :regularized && ent_coeff > 0
        total_loss -= ent_coeff * entropy
    end
    return total_loss, actor_loss, critic_loss
end