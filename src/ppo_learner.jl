using MDPs
import MDPs: preexperiment, postepisode, poststep
using UnPack
using Random
using Flux
using Flux.Zygote
using Flux: Optimiser, ClipNorm

export PPOLearner

Base.@kwdef mutable struct PPOLearner <: AbstractHook
    envs::Vector{AbstractMDP}   # A vector of differently seeded environments.
    actor::Union{Actor, RecurrentActor}       # `Actor` maps states to action probabilities, and the input size of dimensionality of state space. `RecurrentActor` maps catenation of [latest action (onehot), latest reward, current state] to action probabilities, and therefore the input size is: number of actions + 1 + dimensionality of states
    critic                      # Any Flux model mapping ð‘† -> â„. If the actor is a `RecurrentActor`, the critic is also expected to be a recurrent model mapping â„^|ð´| Ã— â„ Ã— ð‘† -> â„. i.e., like for the actor, the inputs are a catenation of [latest action (onehot), latest reward, current state].
    Î³::Float32 = 0.99           # discount factor. Used to calulate TD(Î») advantages.
    nsteps::Int = 100           # numer of steps per iteration. So that total data per iteration = nenvs * nsteps. With longer nsteps, TD(Î») returns are computed better.
    nepochs::Int = 10           # number of epochs per iteration.
    batch_size::Int = 256       # will be clipped to be between nenvs and nenvs * nsteps. Accordingly, nsteps per batch (which is truncated backprop horizon in case of RNNs) = batch_size / nenvs.
    entropy_bonus::Float32 = 0.01f0 # coefficient of the entropy term in the overall PPO loss, to encourage exploration.
    clipnorm::Float32 = Inf     # clip gradients by norm
    adam_weight_decay::Float32 = 0f0      # adam weight decay
    adam_epsilon::Float32 = 1f-7    # adam epsilon
    lr::Float32 = 0.0003        # adam learning rate
    Î» = 0.95f0                  # Used to calulate TD(Î») advantages using Generalized Advantage Estimation (GAE) method.
    Ïµ = 0.2f0                   # epsilon used in PPO clip objective
    kl_target = 0.01            # In each iteration, early stop training if KL divergence from old policy exceeds this value.
    ppo = true                  # whether to use PPO clip objective. If false, standard advantange actor-critic (A2C) objective will be used.
    device = cpu                # `cpu` or `gpu`

    # data structures:
    optim = make_adam_optim(lr, (0.9, 0.999), adam_epsilon, clipnorm, adam_weight_decay)
    actor_gpu = device(deepcopy(actor))
    critic_gpu = device(deepcopy(critic))
    ð¡â‚œ = nothing                # hidden states of the RNNs between training iterations
    ð¬â‚œ = nothing                # states of envs between training iterations
    ð¯â‚œ = nothing                # values of the states of the envs between training iterations
    stats = Dict{Symbol, Any}()
end


function postepisode(ppo::PPOLearner; returns, steps, rng, kwargs...)
    @unpack envs, Î³, nsteps, nepochs, batch_size, entropy_bonus, Î», Ïµ, kl_target, device = ppo

    isrecurrent = ppo.actor isa RecurrentActor
    m = size(state_space(envs[1]), 1)
    n = size(action_space(envs[1]), 1)
    N = length(envs)
    M = nsteps
    episodes = length(returns)

    if isnothing(ppo.ð¬â‚œ)
        Threads.@threads for i in 1:N; reset!(envs[i], rng=rng); end
        ð¬â‚œ = mapfoldl(state, hcat, envs) |> tof32
        if isrecurrent
            ð¬â‚œ = vcat(zeros(Float32, n, N), zeros(Float32, 1, N), ð¬â‚œ)
        end
        reset_rnn_state!.((ppo.actor, ppo.critic))
        init_ð¬â‚œ = ð¬â‚œ
        init_ð¡â‚œ = get_rnn_state.((ppo.actor, ppo.critic))
        init_ð¯â‚œ = ppo.critic(ð¬â‚œ)
    else
        init_ð¬â‚œ = ppo.ð¬â‚œ
        init_ð¡â‚œ = ppo.ð¡â‚œ
        init_ð¯â‚œ = ppo.ð¯â‚œ
    end

    function collect_trajectories(actor, critic)    
        ð¬â‚œ = init_ð¬â‚œ # start where we left off
        isrecurrent && set_rnn_state!.((actor, critic), init_ð¡â‚œ)
        ð¯â‚œ = init_ð¯â‚œ
        data = map(1:M) do t
            ð›‘â‚œ, logð›‘â‚œ = get_probs_logprobs(actor, ð¬â‚œ)   # Forward actor
            ðšâ‚œ = fill(CartesianIndex(0, 0), N)
            Threads.@threads for i in 1:N
                aáµ¢â‚œ = sample(rng, 1:n, ProbabilityWeights(ð›‘â‚œ[:, i]))
                step!(envs[i], aáµ¢â‚œ; rng=rng)
                ðšâ‚œ[i] = CartesianIndex(aáµ¢â‚œ, i)
            end
            ð«â‚œ = mapfoldl(reward, hcat, envs) |> tof32
            ð¬â‚œâ€²= mapfoldl(state, hcat, envs) |> tof32
            ðâ‚œ = mapfoldl(in_absorbing_state, hcat, envs) |> tof32
            ð­â‚œ = mapfoldl(truncated, hcat, envs) |> tof32
            if isrecurrent
                ðšâ‚œ_onehot = zeros(Float32, n, N)
                ðšâ‚œ_onehot[ðšâ‚œ] .= 1
                ð¬â‚œâ€² = vcat(ðšâ‚œ_onehot, ð«â‚œ, ð¬â‚œâ€²)
                ð¡â‚œ_backup = get_rnn_state(critic)  # create a backup
            end
            ð¯â‚œâ€² = critic(ð¬â‚œâ€²)
            ð›…â‚œ = ð«â‚œ + Î³ * (1f0 .- ðâ‚œ) .* ð¯â‚œâ€² - ð¯â‚œ
            dataâ‚œ = (ð¬â‚œ, ðšâ‚œ, ð«â‚œ, ðâ‚œ, ð­â‚œ, ð¬â‚œâ€², ð›…â‚œ, ð›‘â‚œ, logð›‘â‚œ, ð¯â‚œ)
            # ---------------- prepare of next step -------------------
            # set up states:
            ð¬â‚œ = copy(ð¬â‚œâ€²)
            any_reset = false
            for i in 1:N
                if ðâ‚œ[1, i] + ð­â‚œ[1, i] > 0
                    reset!(envs[i]; rng=rng);
                    ð¬â‚œ[:, i] .= 0f0
                    ð¬â‚œ[end-m+1:end, i] .= tof32(state(envs[i]))
                    any_reset = true
                end
            end
            # setup rnn states:
            if isrecurrent && any_reset
                set_rnn_state!(critic, ð¡â‚œ_backup)
                reset_idxs::BitVector = ((ðâ‚œ + ð­â‚œ) .> 0)[1, :]
                reset_rnn_state!.((actor, critic), (reset_idxs, reset_idxs));
            end
            ð¯â‚œ = any_reset ? critic(ð¬â‚œ) : ð¯â‚œâ€²
            # ---------------------------------------------------------
            return dataâ‚œ
        end
        ppo.ð¬â‚œ = ð¬â‚œ
        ppo.ð¡â‚œ = get_rnn_state.((actor, critic))
        ppo.ð¯â‚œ = ð¯â‚œ
        return data
    end
        
    # update advantages using GAE
    function update_advantates!(data)
        ð€â‚œâ€² = 0
        for dataâ‚œ in reverse(data)
            (ð¬â‚œ, ðšâ‚œ, ð«â‚œ, ðâ‚œ, ð­â‚œ, ð¬â‚œâ€², ð›…â‚œ, ð›‘â‚œ, logð›‘â‚œ, ð¯â‚œ) = dataâ‚œ
            ð›…â‚œ .+= Î³ * Î» * (1f0 .- ðâ‚œ) .* ð€â‚œâ€²
            ð€â‚œâ€² = ð›…â‚œ
        end
    end
        
    # update actor critic on entire data. Truncated backprop through time.
    function update_actor_critic_one_epoch!(actor, critic, data)
        â„“, vÌ„, HÌ„, kl = 0, 0, 0, 0
        isrecurrent && set_rnn_state!.((actor, critic), init_ð¡â‚œ)
        Î¸ = Flux.params(actor, critic)
        batch_size = clamp(N, ppo.batch_size, N*M)
        batch_nsteps = batch_size Ã· N
        if !isrecurrent; data = shuffle(rng, data); end
        foreach(splitequal(M, batch_nsteps)) do timeindices
            âˆ‡ = gradient(Î¸) do
                return mapfoldl(+, data[timeindices]) do (ð¬â‚œ, ðšâ‚œ, ð«â‚œ, ðâ‚œ, ð­â‚œ, ð¬â‚œâ€², ð›…â‚œ, oldð›‘â‚œ, oldlogð›‘â‚œ, ð¯â‚œ)
                    ð›‘â‚œ, logð›‘â‚œ = get_probs_logprobs(actor, ð¬â‚œ)
                    if ppo.ppo
                        ð‘Ÿâ‚œ =  ð›‘â‚œ[ðšâ‚œ] ./ oldð›‘â‚œ[ðšâ‚œ]
                        actor_lossâ‚œ = -min.(ð‘Ÿâ‚œ .* ð›…â‚œ, clamp.(ð‘Ÿâ‚œ, 1-Ïµ, 1+Ïµ) .* ð›…â‚œ) |> mean
                    else
                        actor_lossâ‚œ = -ð›…â‚œ .* logð›‘â‚œ[ðšâ‚œ] |> mean
                    end
                    critic_lossâ‚œ = Flux.mse(critic(ð¬â‚œ), ð¯â‚œ + ð›…â‚œ)
                    HÌ„â‚œ = -sum(ð›‘â‚œ .* logð›‘â‚œ; dims=1) |> mean
                    lossâ‚œ = actor_lossâ‚œ + critic_lossâ‚œ - entropy_bonus * HÌ„â‚œ

                    if isrecurrent
                        reset_idxs::BitVector = Zygote.@ignore (cpu(ðâ‚œ + ð­â‚œ) .> 0)[1, :]
                        reset_rnn_state!.((actor, critic), (reset_idxs, reset_idxs));
                    end

                    HÌ„ += Zygote.@ignore HÌ„â‚œ / M
                    â„“ += Zygote.@ignore lossâ‚œ / M
                    vÌ„ += Zygote.@ignore mean(ð¯â‚œ) / M
                    kl += Zygote.@ignore kldivergence(oldð›‘â‚œ, ð›‘â‚œ) / M

                    return lossâ‚œ
                end
            end
            Flux.update!(ppo.optim, Î¸, âˆ‡)
        end
        return â„“, vÌ„, HÌ„, kl
    end

    function calc_kl_div(actor, data)
        kl = 0
        isrecurrent && set_rnn_state!(actor, init_ð¡â‚œ[1])
        return mapfoldl(+, data) do (ð¬â‚œ, ðšâ‚œ, ð«â‚œ, ðâ‚œ, ð­â‚œ, ð¬â‚œâ€², ð›…â‚œ, oldð›‘â‚œ, oldlogð›‘â‚œ, ð¯â‚œ)
            ð›‘â‚œ, logð›‘â‚œ = get_probs_logprobs(actor, ð¬â‚œ)
            isrecurrent && reset_rnn_state!(actor, (cpu(ðâ‚œ + ð­â‚œ) .> 0)[1, :]);
            return kldivergence(oldð›‘â‚œ, ð›‘â‚œ) / M
        end
        return â„“, vÌ„, HÌ„, kl
    end

    function update_actor_critic_with_early_stopping!(actor, critic, data, epochs)
        â„“, vÌ„, HÌ„, kl = 0, 0, 0, 0
        num_epochs = 0
        for epoch in 1:epochs
            â„“, vÌ„, HÌ„, kl = update_actor_critic_one_epoch!(actor, critic, data)
            num_epochs += 1
            kl = calc_kl_div(actor, data)
            kl >= kl_target && break
        end
        return â„“, vÌ„, HÌ„, kl, num_epochs
    end

    data = collect_trajectories(ppo.actor, ppo.critic)
    update_advantates!(data)
    if ppo.device == gpu; data = device(map(dataâ‚œ -> ppo.device.(dataâ‚œ), data)); end;

    Flux.loadparams!(ppo.actor_gpu.actor_model, Flux.params(ppo.actor.actor_model))
    Flux.loadparams!(ppo.critic_gpu, Flux.params(ppo.critic))

    â„“, vÌ„, HÌ„, kl, num_epochs = update_actor_critic_with_early_stopping!(ppo.actor_gpu, ppo.critic_gpu, data, nepochs)

    Flux.loadparams!(ppo.actor.actor_model, Flux.params(ppo.actor_gpu.actor_model))
    Flux.loadparams!(ppo.critic, Flux.params(ppo.critic_gpu))

    ppo.stats[:â„“] = â„“
    ppo.stats[:HÌ„] = HÌ„
    ppo.stats[:kl] = kl
    ppo.stats[:vÌ„] = vÌ„
    ppo.stats[:num_epochs] = num_epochs

    @debug "learning stats" steps episodes stats...
    nothing
end