using MDPs
using StatsBase
using Flux
using Random

export PPOActor, PPOActorDiscrete, PPOActorContinuous, RecurrenceType, MARKOV, RECURRENT, TRANSFORMER

@enum RecurrenceType MARKOV RECURRENT TRANSFORMER


mutable struct PPOActorDiscrete{T<:AbstractFloat} <: AbstractPolicy{Vector{T}, Int}
    const recurtype::RecurrenceType
    actor_model  # maps states to action log probabilities
    deterministic::Bool
    const n::Int # number of actions

    const observation_history::Vector{Vector{Float32}}
    const misc_args
end
function PPOActorDiscrete{T}(recurtype::RecurrenceType, actor_model, deterministic::Bool, n::Int, misc_args) where {T}
    return PPOActorDiscrete{T}(recurtype, actor_model, deterministic, n, Vector{Float32}[], misc_args)
end

Flux.@functor PPOActorDiscrete (actor_model, )
Flux.gpu(p::PPOActorDiscrete{T}) where {T}  = PPOActorDiscrete{T}(p.recurtype, Flux.gpu(p.actor_model), p.deterministic, p.n, p.observation_history, p.misc_args)
Flux.cpu(p::PPOActorDiscrete{T}) where {T}  = PPOActorDiscrete{T}(p.recurtype, Flux.cpu(p.actor_model), p.deterministic, p.n, p.observation_history, p.misc_args)



function (p::PPOActorDiscrete{T})(rng::AbstractRNG, ğ¬::AbstractArray{Float32})::VecOrMat{Int} where {T}
    probabilities, logprobabilities = get_probs_logprobs(p, ğ¬)
    if ndims(probabilities) == 3
        n, seq_len, batch_size = size(probabilities)
        ğš = zeros(Int, seq_len, batch_size)
        for i in 1:batch_size
            for j in 1:seq_len
                ğš[j, i] = sample(rng, 1:n, ProbabilityWeights(probabilities[:, j, i]))
            end
        end
    else
        n, batch_size = size(probabilities)
        ğš = zeros(Int, batch_size)
        for i in 1:batch_size
            ğš[i] = sample(rng, 1:n, ProbabilityWeights(probabilities[:, i]))
        end
    end
    return ğš
end

function (p::PPOActorDiscrete{T})(ğ¬::AbstractArray{Float32}, ğš::AbstractArray{Int})::AbstractArray{Float32} where {T}
    get_probs_logprobs(p, ğ¬, ğš)[1]
end

function get_probs_logprobs(p::PPOActorDiscrete{T}, ğ¬::AbstractArray{Float32}, ğš::AbstractArray{Int})::Tuple{AbstractArray{Float32}, AbstractArray{Float32}} where {T}
    probabilities, logprobabilities = get_probs_logprobs(p, ğ¬)
    @assert ndims(ğš) == ndims(probabilities) - 1
    if ndims(probabilities) == 3
        seq_len, batch_size = size(ğš)
        ğš = Flux.Zygote.@ignore [CartesianIndex(ğš[1, t, i], t, i) for t in 1:seq_len, i in 1:batch_size]
    else
        batch_size = length(ğš)
        ğš = Flux.Zygote.@ignore [CartesianIndex(ğš[i], i) for i in 1:batch_size]
    end
    return probabilities[ğš], logprobabilities[ğš]
end

function get_probs_logprobs(p::PPOActorDiscrete{T}, ğ¬::AbstractArray{Float32})::Tuple{AbstractArray{Float32}, AbstractArray{Float32}} where {T}
    if p.recurtype âˆˆ (MARKOV, TRANSFORMER) || ndims(ğ¬) == 2
        logits = p.actor_model(ğ¬)
    else
        # interpret as (state_dim, ntimesteps, batch_size) for the RNN
        Flux.Zygote.@ignore Flux.reset!(p.actor_model)
        logits = mapfoldl(hcat, 1:size(ğ¬, 2)) do t
            return reshape(p.actor_model(ğ¬[:, t, :]), :, 1, size(ğ¬, 3))
        end
    end
    if p.deterministic
        logits = logits * 1.0f6
    end
    probabilities = Flux.softmax(logits)
    logprobabilities = Flux.logsoftmax(logits)
    return probabilities, logprobabilities
end

function get_entropy(p::PPOActorDiscrete{T}, ğ¬::AbstractArray{Float32})::Float32 where {T}
    ğ›‘, logğ›‘ = get_probs_logprobs(p, ğ¬)
    return -sum(ğ›‘ .* logğ›‘; dims=1) |> mean
end

function get_entropy(p::PPOActorDiscrete{T}, ğ›‘, logğ›‘)::Float32 where {T}
    return -sum(ğ›‘ .* logğ›‘; dims=1) |> mean
end

function get_kl_div(p::PPOActorDiscrete, ğ¬, ğš, oldğ›‘, oldlogğ›‘)
    ğ›‘, logğ›‘ = get_probs_logprobs(p, ğ¬)
    return sum(oldğ›‘ .* (oldlogğ›‘ .- logğ›‘); dims=1) |> mean
end


function get_loss_and_entropy(p::PPOActorDiscrete, ğ¬, ğš, ğ›…, oldğ›‘, oldlogğ›‘, Ïµ, use_clip_objective=true)
    _, seq_len, batch_size = size(ğš)
    ğš = Flux.Zygote.@ignore [CartesianIndex(ğš[1, t, i], t, i) for t in 1:seq_len, i in 1:batch_size]
    ğš = reshape(ğš, 1, seq_len, batch_size)
    ğ›‘, logğ›‘ = get_probs_logprobs(p, ğ¬)
    if use_clip_objective
        ğ‘Ÿ =  ğ›‘[ğš] ./ oldğ›‘[ğš]
        loss = -min.(ğ‘Ÿ .* ğ›…, clamp.(ğ‘Ÿ, 1f0-Ïµ, 1f0+Ïµ) .* ğ›…) |> mean
    else
        loss = -ğ›… .* logğ›‘[ğš] |> mean
    end
    entropy = get_entropy(p, ğ›‘, logğ›‘)
    return loss, entropy
end







mutable struct PPOActorContinuous{Tâ‚› <: AbstractFloat, Tâ‚ <: AbstractFloat} <: AbstractPolicy{Vector{Tâ‚›}, Vector{Tâ‚}}
    const recurtype::RecurrenceType
    actor_model  # maps states to action log probabilities
    deterministic::Bool
    
    logstd::AbstractVector{Float32}
    shift::AbstractVector{Float32}
    scale::AbstractVector{Float32}

    const observation_history::Vector{Vector{Float32}}
    const misc_args
end
function PPOActorContinuous{Tâ‚›, Tâ‚}(recurtype::RecurrenceType, actor_model, deterministic::Bool, aspace::VectorSpace{Tâ‚}, misc_args) where {Tâ‚›, Tâ‚}
    n = size(aspace, 1)
    logstd = zeros(n) |> tof32
    shift = (aspace.lows + aspace.highs) / 2  |> tof32
    scale = (aspace.highs - aspace.lows) / 2  |> tof32
    return PPOActorContinuous{Tâ‚›, Tâ‚}(recurtype, actor_model, deterministic, logstd, shift, scale, Vector{Float32}[], misc_args)
end

Flux.@functor PPOActorContinuous (actor_model, logstd)
Flux.gpu(p::PPOActorContinuous{Tâ‚›, Tâ‚}) where {Tâ‚›, Tâ‚}  = PPOActorContinuous{Tâ‚›, Tâ‚}(p.recurtype, Flux.gpu(p.actor_model), p.deterministic, Flux.gpu(p.logstd), Flux.gpu(p.shift), Flux.gpu(p.scale), p.observation_history, p.misc_args)
Flux.cpu(p::PPOActorContinuous{Tâ‚›, Tâ‚}) where {Tâ‚›, Tâ‚}  = PPOActorContinuous{Tâ‚›, Tâ‚}(p.recurtype, Flux.cpu(p.actor_model), p.deterministic, Flux.cpu(p.logstd), Flux.cpu(p.shift), Flux.cpu(p.scale), p.observation_history, p.misc_args)


function (p::PPOActorContinuous{Tâ‚›, Tâ‚})(rng::AbstractRNG, ğ¬::AbstractArray{Float32})::AbstractArray{Float32} where {Tâ‚›, Tâ‚}
    ğš, logğ›‘ğš = sample_action_logprobs(p, rng, ğ¬)
    return ğš
end

function (p::PPOActorContinuous{Tâ‚›, Tâ‚})(ğ¬::AbstractArray{Float32}, ğš::AbstractArray{Float32})::AbstractArray{Float32} where {Tâ‚›, Tâ‚}
    get_logprobs(p, ğ¬, ğš)
end

function sample_action_logprobs(p::PPOActorContinuous{Tâ‚›, Tâ‚}, rng::AbstractRNG, ğ¬::AbstractArray{Float32})::Tuple{AbstractArray{Float32}, AbstractArray{Float32}} where {Tâ‚›, Tâ‚}
    ğ› = p.actor_model(ğ¬)
    logğ›” = clamp.(p.logstd, -20f0, 2f0)
    ğ›” = (1f0 - Float32(p.deterministic)) * exp.(logğ›”)
    ğ› = Flux.Zygote.@ignore convert(typeof(ğ›), randn(rng, Float32, size(ğ›)))
    ğš = ğ› .+ ğ›” .* ğ›
    logğ›‘ğš = sum(log_nomal_prob.(ğš, ğ›, ğ›”), dims=1)
    ğš = tanh.(ğš)
    logğ›‘ğš = logğ›‘ğš .- sum(log.(1f0 .- ğš .^ 2 .+ 1f-6), dims=1)
    ğš = p.shift .+ p.scale .* ğš
    return ğš, logğ›‘ğš
end


function get_logprobs(p::PPOActorContinuous{Tâ‚›, Tâ‚}, ğ¬::AbstractArray{Float32}, ğš::AbstractArray{Float32})::AbstractArray{Float32} where {Tâ‚›, Tâ‚}
    ğš = clamp.(ğš, -1f0 + 1f-6, 1f0 - 1f-6)
    ğš_unshifted_unscaled = (ğš .- p.shift) ./ p.scale
    ğš_untanhed = atanh.(ğš_unshifted_unscaled)

    if p.recurtype âˆˆ (MARKOV, TRANSFORMER) || ndims(ğ¬) == 2
        ğ› = p.actor_model(ğ¬)
    else
        # interpret as (state_dim, ntimesteps, batch_size)
        Flux.Zygote.@ignore Flux.reset!(p.actor_model)
        ğ› = mapfoldl(hcat, 1:size(ğ¬, 2)) do t
            reshape(p.actor_model(ğ¬[:, t, :]), 1, 1, size(ğ¬, 3))
        end
    end
    ğ› = p.actor_model(ğ¬)
    logğ›” = clamp.(p.logstd, -20f0, 2f0)
    ğ›” = (1f0 - Float32(p.deterministic)) * exp.(logğ›”)

    logğ›‘ğš = sum(log_nomal_prob.(ğš_untanhed, ğ›, ğ›”), dims=1)
    logğ›‘ğš = logğ›‘ğš .- sum(log.(1f0 .- ğš_unshifted_unscaled .^ 2 .+ 1f-6), dims=1)

    return logğ›‘ğš
end

function get_entropy(p::PPOActorContinuous)
    D = length(p.logstd)
    logÏƒ = clamp.(p.logstd, -20f0, 2f0)
    return 0.5f0 * D * (1f0 + LOG_2PI) + sum(logÏƒ)
end

function get_entropy(p::PPOActorContinuous, ğ¬)
    get_entropy(p)
end

function get_kl_div(p::PPOActorContinuous, ğ¬, ğš, oldğ›‘, oldlogğ›‘)
    logğ›‘ = get_logprobs(p, ğ¬, ğš)
    logratio = logğ›‘ .- oldlogğ›‘
    kl = (exp.(logratio) .- 1f0 .- logratio) |> mean  # More stable than -logratio. http://joschu.net/blog/kl-approx.html
    return kl
end


function get_loss_and_entropy(p::PPOActorContinuous, ğ¬, ğš, ğ›…, oldğ›‘, oldlogğ›‘, Ïµ, use_clip_objective=true)
    logğ›‘ = get_logprobs(p, ğ¬, ğš)
    ğ‘Ÿ = exp.(logğ›‘ .- oldlogğ›‘)
    if use_clip_objective
        loss = -min.(ğ‘Ÿ .* ğ›…, clamp.(ğ‘Ÿ, 1f0 - Ïµ, 1f0 + Ïµ) .* ğ›…) |> mean
    else
        loss = ğ›… .* -logğ›‘ |> mean
    end
    entropy = get_entropy(p)
    return loss, entropy
end





const PPOActor{Tâ‚›, Tâ‚} = Union{PPOActorContinuous{Tâ‚›, Tâ‚}, PPOActorDiscrete{Tâ‚›}}


function MDPs.preepisode(p::PPOActor; env, kwargs...)
    Flux.reset!(p.actor_model)
    empty!(p.observation_history)
    p.recurtype == TRANSFORMER && push!(p.observation_history, tof32(deepcopy(state(env))))
    nothing
end

function (p::PPOActorDiscrete{Tâ‚›})(rng::AbstractRNG, s::Vector{Tâ‚›})::Int where {Tâ‚›}
    ppo_unified(p, rng, s)
end
function (p::PPOActorDiscrete{Tâ‚›})(s::Vector{Tâ‚›}, a::Int)::Float64 where {Tâ‚›}
    ppo_unified(p, s, a)
end

function (p::PPOActorContinuous{Tâ‚›, Tâ‚})(rng::AbstractRNG, s::Vector{Tâ‚›})::Vector{Tâ‚} where {Tâ‚›, Tâ‚}
    ppo_unified(p, rng, s)
end
function (p::PPOActorContinuous{Tâ‚›, Tâ‚})(s::Vector{Tâ‚›}, a::Vector{Tâ‚})::Float64 where {Tâ‚›, Tâ‚}
    ppo_unified(p, s, a)
end

function ppo_unified(p::PPOActor{Tâ‚›, Tâ‚}, rng::AbstractRNG, s::Vector{Tâ‚›}) where {Tâ‚›, Tâ‚}
    if p.recurtype == TRANSFORMER
        push!(p.observation_history, tof32(deepcopy(s)))
        s = hcat(p.observation_history...)
        # some ugly stuff: TODO: put this into a function or some wrapper.
        # sl = size(s, 2)
        # task_horizon, horizon, extrapolate_to = p.misc_args
        # _CL = decide_context_length(sl, extrapolate_to, horizon, task_horizon)
        # if sl > _CL
        #     if sl % task_horizon == 0
        #         cl = _CL
        #     else
        #         cl = _CL - task_horizon + sl % task_horizon
        #     end
        #     s = s[:, end-cl+1:end]
        #     tstart = s[end, 1]
        #     s[end, :] = s[end, :] .- tstart
        #     s[end, :] = (extrapolate_to/horizon) * s[end, :]
        # end
        # end of ugly stuff.
    end
    ğ¬ = s |> batch |> tof32
    a = p(rng, ğ¬) |> unbatch
    if p.recurtype == TRANSFORMER
        a = unbatch_last(a)
    end
    return a
end

function ppo_unified(p::PPOActor{Tâ‚›, Tâ‚}, s::Vector{Tâ‚›}, a::Union{Int, Vector{Tâ‚}})::Float64 where {Tâ‚›, Tâ‚}
    if p.recurtype == TRANSFORMER
        s = hcat(p.observation_history...)
    end
    ğ¬ = s |> batch |> tof32
    ğš = a |> batch
    Ï€a = p(ğ¬, ğš) |> unbatch
    if p.recurtype == TRANSFORMER
        Ï€a = unbatch_last(Ï€a)
    end
    return Ï€a
end

