using MDPs
using StatsBase
using Flux
using Random

export PPOActorDiscrete, PPOActorContinuous, RecurrenceType, MARKOV, RECURRENT, TRANSFORMER

@enum RecurrenceType MARKOV RECURRENT TRANSFORMER


mutable struct PPOActorDiscrete{T<:AbstractFloat} <: AbstractPolicy{Vector{T}, Int}
    const recurtype::RecurrenceType
    actor_model  # maps states to action logits
    deterministic::Bool
    const n::Int # number of actions

    const observation_history::Vector{Vector{Float32}}
end

"""
    PPOActorDiscrete{T}(actor_model, deterministic::Bool, aspace::IntegerSpace, recurtype::RecurrenceType=MARKOV) where {T}

Construct a PPOActorDiscrete policy, where the states are of type Vector{T}. The policy is parameterized by a Flux model, which can be recurrent or a transformer to handle sequential inputs. The policy can be stochastic or deterministic.

# Arguments
- `actor_model`: a Flux model that maps states to action logits.
- `deterministic`: whether the policy is deterministic or stochastic. If deterministic, the policy will always output the action with the highest probability. If stochastic, the policy will sample actions from the probability distribution over actions.
- `aspace`: the action space
- `recurtype`: the recurrence type of the neural network (MARKOV, RECURRENT, TRANSFORMER). Defaults to MARKOV (no recurrence).
"""
function PPOActorDiscrete{T}(actor_model, deterministic::Bool, aspace::IntegerSpace, recurtype::RecurrenceType=MARKOV) where {T}
    return PPOActorDiscrete{T}(recurtype, actor_model, deterministic, length(aspace), Vector{Float32}[])
end

Flux.@functor PPOActorDiscrete (actor_model, )
Flux.gpu(p::PPOActorDiscrete{T}) where {T}  = PPOActorDiscrete{T}(p.recurtype, Flux.gpu(p.actor_model), p.deterministic, p.n, p.observation_history)
Flux.cpu(p::PPOActorDiscrete{T}) where {T}  = PPOActorDiscrete{T}(p.recurtype, Flux.cpu(p.actor_model), p.deterministic, p.n, p.observation_history)

"""
    (p::PPOActorDiscrete{T})(rng::AbstractRNG, ğ¬::AbstractArray{Float32})::VecOrMat{Int} where {T}

Sample actions from the policy given the states ğ¬. States ğ¬ are assumed to be in the form (state_dim, ntimesteps, batch_size) or (state_dim, batch_size).
# Returns
- `ğš`: the sampled actions as a Vector{Int} for markov policies and a Matrix{Int} for recurrent policies
"""
function (p::PPOActorDiscrete{T})(rng::AbstractRNG, ğ¬::AbstractArray{Float32})::VecOrMat{Int} where {T}
    probabilities, logprobabilities = get_probs_logprobs(p, ğ¬)
    if ndims(probabilities) == 3
        n, seq_len, batch_size = size(probabilities)
        ğš = zeros(Int, seq_len, batch_size)
        for i in 1:batch_size
            for j in 1:seq_len
                ğš[j, i] = sample(rng, 1:n, ProbabilityWeights(probabilities[:, j, i]))
            end
        end
    else
        n, batch_size = size(probabilities)
        ğš = zeros(Int, batch_size)
        for i in 1:batch_size
            ğš[i] = sample(rng, 1:n, ProbabilityWeights(probabilities[:, i]))
        end
    end
    return ğš
end

"""
    (p::PPOActorDiscrete{T})(ğ¬::AbstractArray{Float32}, ğš::AbstractArray{Int})::AbstractArray{Float32} where {T}

Get the probabilities of the actions ğš given the states ğ¬. States ğ¬ are assumed to be in the form (state_dim, ntimesteps, batch_size) or (state_dim, batch_size). Actions are accordingly assumed to be in the form (ntimesteps, batch_size) or (batch_size).

# Returns
- `probabilities`: the probabilities of the actions ğš given the states ğ¬. 

The output is of the same shape as ğš.
"""
function (p::PPOActorDiscrete{T})(ğ¬::AbstractArray{Float32}, ğš::AbstractArray{Int})::AbstractArray{Float32} where {T}
    get_probs_logprobs(p, ğ¬, ğš)[1]
end

"""
    get_probs_logprobs(p::PPOActorDiscrete, ğ¬::AbstractArray{Float32}, ğš::AbstractArray{Int})::Tuple{AbstractArray{Float32}, AbstractArray{Float32}}

Get the probabilities and log probabilities of the actions ğš given the states ğ¬. States ğ¬ are assumed to be in the form (state_dim, ntimesteps, batch_size) or (state_dim, batch_size). Actions are accordingly assumed to be in the form (ntimesteps, batch_size) or (batch_size).

# Returns
- `probabilities`: the probabilities of the actions ğš given the states ğ¬.
- `logprobabilities`: the log probabilities of the actions ğš given the states ğ¬

The outputs are of the same shape as ğš.
"""
function get_probs_logprobs(p::PPOActorDiscrete{T}, ğ¬::AbstractArray{Float32}, ğš::AbstractArray{Int})::Tuple{AbstractArray{Float32}, AbstractArray{Float32}} where {T}
    probabilities, logprobabilities = get_probs_logprobs(p, ğ¬)
    @assert ndims(ğš) == ndims(probabilities) - 1
    if ndims(probabilities) == 3
        seq_len, batch_size = size(ğš)
        ğš = Flux.Zygote.@ignore [CartesianIndex(ğš[1, t, i], t, i) for t in 1:seq_len, i in 1:batch_size]
    else
        batch_size = length(ğš)
        ğš = Flux.Zygote.@ignore [CartesianIndex(ğš[i], i) for i in 1:batch_size]
    end
    return probabilities[ğš], logprobabilities[ğš]
end

"""
    get_probs_logprobs(p::PPOActorDiscrete, ğ¬::AbstractArray{Float32})::Tuple{AbstractArray{Float32}, AbstractArray{Float32}}

Get the probabilities and log probabilities of all actions given the states ğ¬. States ğ¬ are assumed to be in the form (state_dim, ntimesteps, batch_size) or (state_dim, batch_size).

# Returns
- `probabilities`: the probabilities of all actions given the states ğ¬.
- `logprobabilities`: the log probabilities of all actions given the states ğ¬

The outputs are of the shape (n, ntimesteps, batch_size) or (n, batch_size) depending on the shape of ğ¬, where n is the number of actions.
"""
function get_probs_logprobs(p::PPOActorDiscrete{T}, ğ¬::AbstractArray{Float32})::Tuple{AbstractArray{Float32}, AbstractArray{Float32}} where {T}
    if p.recurtype âˆˆ (MARKOV, TRANSFORMER) || ndims(ğ¬) == 2
        logits = p.actor_model(ğ¬)
    else
        # interpret as (state_dim, ntimesteps, batch_size) for the RNN
        Flux.Zygote.@ignore Flux.reset!(p.actor_model)
        logits = mapfoldl(hcat, 1:size(ğ¬, 2)) do t
            return reshape(p.actor_model(ğ¬[:, t, :]), :, 1, size(ğ¬, 3))
        end
    end
    if p.deterministic
        logits = logits * 1.0f6
    end
    probabilities = Flux.softmax(logits)
    logprobabilities = Flux.logsoftmax(logits)
    return probabilities, logprobabilities
end

"""
    get_entropy(p::PPOActorDiscrete, ğ¬::AbstractArray{Float32})::Float32 where {T}

Get the entropy of the policy given the states ğ¬
"""
function get_entropy(p::PPOActorDiscrete{T}, ğ¬::AbstractArray{Float32})::Float32 where {T}
    ğ›‘, logğ›‘ = get_probs_logprobs(p, ğ¬)
    return -sum(ğ›‘ .* logğ›‘; dims=1) |> mean
end

"""
    get_entropy(p::PPOActorDiscrete{T}, ğ›‘, logğ›‘)::Float32 where {T}

Get the entropy given the probabilities ğ›‘ and log probabilities logğ›‘.
"""
function get_entropy(p::PPOActorDiscrete{T}, ğ›‘, logğ›‘)::Float32 where {T}
    return -sum(ğ›‘ .* logğ›‘; dims=1) |> mean
end

"""
    get_kl_div(p::PPOActorDiscrete, ğ¬, ğš, oldğ›‘, oldlogğ›‘)

Get the KL divergence between the old policy and the current policy given the states ğ¬, actions ğš, and old probabilities oldğ›‘ and old log probabilities oldlogğ›‘ for actions ğš in states ğ¬
"""
function get_kl_div(p::PPOActorDiscrete, ğ¬, ğš, oldğ›‘, oldlogğ›‘)
    ğ›‘, logğ›‘ = get_probs_logprobs(p, ğ¬)
    return sum(oldğ›‘ .* (oldlogğ›‘ .- logğ›‘); dims=1) |> mean
end


function get_loss_and_entropy(p::PPOActorDiscrete, ğ¬, ğš, ğ›…, oldğ›‘, oldlogğ›‘, Ïµ, use_clip_objective=true)
    if ndims(ğ¬) == 3
        _, seq_len, batch_size = size(ğš)
        ğš = Flux.Zygote.@ignore [CartesianIndex(ğš[1, t, i], t, i) for t in 1:seq_len, i in 1:batch_size]
        ğš = reshape(ğš, 1, seq_len, batch_size)
    else
        _, batch_size = size(ğš)
        ğš = Flux.Zygote.@ignore [CartesianIndex(ğš[1, i], i) for i in 1:batch_size]
        ğš = reshape(ğš, 1, batch_size)
    end
    ğ›‘, logğ›‘ = get_probs_logprobs(p, ğ¬)
    if use_clip_objective
        ğ‘Ÿ =  ğ›‘[ğš] ./ oldğ›‘[ğš]
        loss = -min.(ğ‘Ÿ .* ğ›…, clamp.(ğ‘Ÿ, 1f0-Ïµ, 1f0+Ïµ) .* ğ›…) |> mean
    else
        loss = -ğ›… .* logğ›‘[ğš] |> mean
    end
    entropy = get_entropy(p, ğ›‘, logğ›‘)
    return loss, entropy
end







mutable struct PPOActorContinuous{Tâ‚› <: AbstractFloat, Tâ‚ <: AbstractFloat} <: AbstractPolicy{Vector{Tâ‚›}, Vector{Tâ‚}}
    const recurtype::RecurrenceType
    actor_model  # maps states to mean of action distribution
    deterministic::Bool
    
    logstd::AbstractVector{Float32}
    shift::AbstractVector{Float32}
    scale::AbstractVector{Float32}

    const observation_history::Vector{Vector{Float32}}
end

"""
    PPOActorContinuous{Tâ‚›, Tâ‚}(actor_model, deterministic::Bool, aspace::VectorSpace{Tâ‚}, recurtype::RecurrenceType=MARKOV) where {Tâ‚›, Tâ‚}

Create a continuous actor for PPO with a continuous action space. The states and actions are of type `Vector{Tâ‚›}` and `Vector{Tâ‚}` respectively. The actor model maps states to the mean of the action distribution, and can be a recurrent neural network or a transformer to handle sequential inputs. The standard deviation of the action distribution is a learnable parameter. The action is sampled from a normal distribution with the mean and standard deviation. The action is then squashed using the tanh function and scaled and shifted to fit the action space.

# Arguments
- `actor_model`: the Flux model
- `deterministic`: whether to sample actions from the distribution (deterministic=false) or simply take the mean of the distribution (deterministic=true).
- `aspace`: the action space
- `recurtype`: the recurrence type, either `MARKOV`, `RECURRENT`, or `TRANSFORMER`. Defaults to `MARKOV` (no recurrence).
"""
function PPOActorContinuous{Tâ‚›, Tâ‚}(actor_model, deterministic::Bool, aspace::VectorSpace{Tâ‚}, recurtype::RecurrenceType=MARKOV) where {Tâ‚›, Tâ‚}
    n = size(aspace, 1)
    logstd = zeros(n) |> tof32
    shift = (aspace.lows + aspace.highs) / 2  |> tof32
    scale = (aspace.highs - aspace.lows) / 2  |> tof32
    return PPOActorContinuous{Tâ‚›, Tâ‚}(recurtype, actor_model, deterministic, logstd, shift, scale, Vector{Float32}[])
end

Flux.@functor PPOActorContinuous (actor_model, logstd)
Flux.gpu(p::PPOActorContinuous{Tâ‚›, Tâ‚}) where {Tâ‚›, Tâ‚}  = PPOActorContinuous{Tâ‚›, Tâ‚}(p.recurtype, Flux.gpu(p.actor_model), p.deterministic, Flux.gpu(p.logstd), Flux.gpu(p.shift), Flux.gpu(p.scale), p.observation_history)
Flux.cpu(p::PPOActorContinuous{Tâ‚›, Tâ‚}) where {Tâ‚›, Tâ‚}  = PPOActorContinuous{Tâ‚›, Tâ‚}(p.recurtype, Flux.cpu(p.actor_model), p.deterministic, Flux.cpu(p.logstd), Flux.cpu(p.shift), Flux.cpu(p.scale), p.observation_history)

"""
    (p::PPOActorContinuous{Tâ‚›, Tâ‚})(rng::AbstractRNG, ğ¬::AbstractArray{Float32})::AbstractArray{Float32} where {Tâ‚›, Tâ‚}

Given states ğ¬, samples and returns actions. If input is of shape (state_dims, ntimesteps, batch_size), then the output is of shape (action_dims, ntimesteps, batch_size). If input is of shape (state_dims, batch_size), then the output is of shape (action_dims, batch_size).
"""
function (p::PPOActorContinuous{Tâ‚›, Tâ‚})(rng::AbstractRNG, ğ¬::AbstractArray{Float32})::AbstractArray{Float32} where {Tâ‚›, Tâ‚}
    ğš, logğ›‘ğš = sample_action_logprobs(p, rng, ğ¬)
    return ğš
end

function (p::PPOActorContinuous{Tâ‚›, Tâ‚})(ğ¬::AbstractArray{Float32}, ğš::AbstractArray{Float32})::AbstractArray{Float32} where {Tâ‚›, Tâ‚}
    get_logprobs(p, ğ¬, ğš)
end

"""
    sample_action_logprobs(p::PPOActorContinuous{Tâ‚›, Tâ‚}, rng::AbstractRNG, ğ¬::AbstractArray{Float32})::Tuple{AbstractArray{Float32}, AbstractArray{Float32}} where {Tâ‚›, Tâ‚}

Given states ğ¬, samples and returns actions and their log probabilities. If input is of shape (state_dims, ntimesteps, batch_size), then outputs are of shape (action_dims, ntimesteps, batch_size) and (1, nsteps, batch_size) respectively. If input is of shape (state_dims, batch_size), then outputs are of shape (action_dims, batch_size) and (1, batch_size) respectively.
"""
function sample_action_logprobs(p::PPOActorContinuous{Tâ‚›, Tâ‚}, rng::AbstractRNG, ğ¬::AbstractArray{Float32})::Tuple{AbstractArray{Float32}, AbstractArray{Float32}} where {Tâ‚›, Tâ‚}
    ğ› = p.actor_model(ğ¬)
    logğ›” = clamp.(p.logstd, -20f0, 2f0)
    ğ›” = (1f0 - Float32(p.deterministic)) * exp.(logğ›”)
    ğ› = Flux.Zygote.@ignore convert(typeof(ğ›), randn(rng, Float32, size(ğ›)))
    ğš = ğ› .+ ğ›” .* ğ›
    logğ›‘ğš = sum(log_nomal_prob.(ğš, ğ›, ğ›”), dims=1)
    ğš = tanh.(ğš)
    logğ›‘ğš = logğ›‘ğš .- sum(log.(1f0 .- ğš .^ 2 .+ 1f-6), dims=1)
    ğš = p.shift .+ p.scale .* ğš
    return ğš, logğ›‘ğš
end

"""
    get_logprobs(p::PPOActorContinuous{Tâ‚›, Tâ‚}, ğ¬::AbstractArray{Float32}, ğš::AbstractArray{Float32})::AbstractArray{Float32} where {Tâ‚›, Tâ‚}

Given states ğ¬ and actions ğš, returns log probabilities of actions. If input is of shape (state_dims, ntimesteps, batch_size), then outputs are of shape (1, nsteps, batch_size). If input is of shape (state_dims, batch_size), then outputs are of shape (1, batch_size).
"""
function get_logprobs(p::PPOActorContinuous{Tâ‚›, Tâ‚}, ğ¬::AbstractArray{Float32}, ğš::AbstractArray{Float32})::AbstractArray{Float32} where {Tâ‚›, Tâ‚}
    @assert all(isfinite.(ğš))
    ğš_unshifted_unscaled = (ğš .- p.shift) ./ p.scale
    ğš_unshifted_unscaled = clamp.(ğš_unshifted_unscaled, -1f0 + 1f-3, 1f0 - 1f-3)    # because atanh(1.0) is infinite
    @assert all(isfinite.(ğš_unshifted_unscaled))
    ğš_untanhed = atanh.(ğš_unshifted_unscaled)
    @assert all(isfinite.(ğš_untanhed))

    if p.recurtype âˆˆ (MARKOV, TRANSFORMER) || ndims(ğ¬) == 2
        ğ› = p.actor_model(ğ¬)
    else
        # interpret as (state_dim, ntimesteps, batch_size)
        Flux.Zygote.@ignore Flux.reset!(p.actor_model)
        ğ› = mapfoldl(hcat, 1:size(ğ¬, 2)) do t
            reshape(p.actor_model(ğ¬[:, t, :]), :, 1, size(ğ¬, 3))
        end
    end
    logğ›” = clamp.(p.logstd, -20f0, 2f0)
    ğ›” = (1f0 - Float32(p.deterministic)) * exp.(logğ›”)

    logğ›‘ğš = sum(log_nomal_prob.(ğš_untanhed, ğ›, ğ›”), dims=1)
    logğ›‘ğš = logğ›‘ğš .- sum(log.(1f0 .- ğš_unshifted_unscaled .^ 2 .+ 1f-6), dims=1)

    return logğ›‘ğš
end

"""
    get_entropy(p::PPOActorContinuous{Tâ‚›, Tâ‚})::Float32 where {Tâ‚›, Tâ‚}

Returns entropy of the policy. 
"""
function get_entropy(p::PPOActorContinuous)
    D = length(p.logstd)
    logÏƒ = clamp.(p.logstd, -20f0, 2f0)
    return 0.5f0 * D * (1f0 + LOG_2PI) + sum(logÏƒ)
end

"""
    get_entropy(p::PPOActorContinuous{Tâ‚›, Tâ‚}, ğ¬::AbstractArray{Float32})::Float32 where {Tâ‚›, Tâ‚}
Returns entropy of the policy. Simply calls `get_entropy(p::PPOActorContinuous{Tâ‚›, Tâ‚})::Float32 where {Tâ‚›, Tâ‚}` since the entropy depends only on the standard deviation of the policy, which is independent of the state.
"""
function get_entropy(p::PPOActorContinuous, ğ¬)
    get_entropy(p)
end

"""
    get_kl_div(p::PPOActorContinuous, ğ¬, ğš, oldğ›‘, oldlogğ›‘)

Returns KL divergence between old policy and current policy given states ğ¬, actions ğš, and old probabilities and log probabilities of taking those actions under the old policy.
"""
function get_kl_div(p::PPOActorContinuous, ğ¬, ğš, oldğ›‘, oldlogğ›‘)
    logğ›‘ = get_logprobs(p, ğ¬, ğš)
    logratio = logğ›‘ .- oldlogğ›‘
    kl = (exp.(logratio) .- 1f0 .- logratio) |> mean  # More stable than -logratio. http://joschu.net/blog/kl-approx.html
    return kl
end


function get_loss_and_entropy(p::PPOActorContinuous, ğ¬, ğš, ğ›…, oldğ›‘, oldlogğ›‘, Ïµ, use_clip_objective=true)
    logğ›‘ = get_logprobs(p, ğ¬, ğš)
    ğ‘Ÿ = exp.(logğ›‘ .- oldlogğ›‘)
    if use_clip_objective
        loss = -min.(ğ‘Ÿ .* ğ›…, clamp.(ğ‘Ÿ, 1f0 - Ïµ, 1f0 + Ïµ) .* ğ›…) |> mean
    else
        loss = ğ›… .* -logğ›‘ |> mean
    end
    entropy = get_entropy(p)
    return loss, entropy
end





const PPOActor{Tâ‚›, Tâ‚} = Union{PPOActorContinuous{Tâ‚›, Tâ‚}, PPOActorDiscrete{Tâ‚›}}


function MDPs.preepisode(p::PPOActor; env, kwargs...)
    Flux.reset!(p.actor_model)
    empty!(p.observation_history)
    p.recurtype == TRANSFORMER && push!(p.observation_history, tof32(deepcopy(state(env))))
    nothing
end

function (p::PPOActorDiscrete{Tâ‚›})(rng::AbstractRNG, s::Vector{Tâ‚›})::Int where {Tâ‚›}
    ppo_unified(p, rng, s)
end
function (p::PPOActorDiscrete{Tâ‚›})(s::Vector{Tâ‚›}, a::Int)::Float64 where {Tâ‚›}
    ppo_unified(p, s, a)
end

function (p::PPOActorContinuous{Tâ‚›, Tâ‚})(rng::AbstractRNG, s::Vector{Tâ‚›})::Vector{Tâ‚} where {Tâ‚›, Tâ‚}
    ppo_unified(p, rng, s)
end
function (p::PPOActorContinuous{Tâ‚›, Tâ‚})(s::Vector{Tâ‚›}, a::Vector{Tâ‚})::Float64 where {Tâ‚›, Tâ‚}
    ppo_unified(p, s, a)
end

function ppo_unified(p::PPOActor{Tâ‚›, Tâ‚}, rng::AbstractRNG, s::Vector{Tâ‚›}) where {Tâ‚›, Tâ‚}
    if p.recurtype == TRANSFORMER
        push!(p.observation_history, tof32(deepcopy(s)))
        s = hcat(p.observation_history...)
    end
    ğ¬ = s |> batch |> tof32
    a = p(rng, ğ¬) |> unbatch
    if p.recurtype == TRANSFORMER
        a = unbatch_last(a)
    end
    return a
end

function ppo_unified(p::PPOActor{Tâ‚›, Tâ‚}, s::Vector{Tâ‚›}, a::Union{Int, Vector{Tâ‚}})::Float64 where {Tâ‚›, Tâ‚}
    if p.recurtype == TRANSFORMER
        s = hcat(p.observation_history...)
    end
    ğ¬ = s |> batch |> tof32
    ğš = a |> batch
    Ï€a = p(ğ¬, ğš) |> unbatch
    if p.recurtype == TRANSFORMER
        Ï€a = unbatch_last(Ï€a)
    end
    return Ï€a
end

