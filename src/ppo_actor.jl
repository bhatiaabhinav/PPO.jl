using MDPs
using StatsBase
using Flux
using Random

export PPOActor, PPOActorDiscrete, PPOActorContinuous, RecurrenceType, MARKOV, RECURRENT, TRANSFORMER

@enum RecurrenceType MARKOV RECURRENT TRANSFORMER


mutable struct PPOActorDiscrete{T<:AbstractFloat} <: AbstractPolicy{Vector{T}, Int}
    const recurtype::RecurrenceType
    actor_model  # maps states to action log probabilities
    deterministic::Bool
    const n::Int # number of actions

    const observation_history::Vector{Vector{Float32}}
    const misc_args
end
function PPOActorDiscrete{T}(recurtype::RecurrenceType, actor_model, deterministic::Bool, n::Int, misc_args) where {T}
    return PPOActorDiscrete{T}(recurtype, actor_model, deterministic, n, Vector{Float32}[], misc_args)
end

Flux.@functor PPOActorDiscrete (actor_model, )
Flux.gpu(p::PPOActorDiscrete{T}) where {T}  = PPOActorDiscrete{T}(p.recurtype, Flux.gpu(p.actor_model), p.deterministic, p.n, p.observation_history, p.misc_args)
Flux.cpu(p::PPOActorDiscrete{T}) where {T}  = PPOActorDiscrete{T}(p.recurtype, Flux.cpu(p.actor_model), p.deterministic, p.n, p.observation_history, p.misc_args)



function (p::PPOActorDiscrete{T})(rng::AbstractRNG, ùê¨::AbstractArray{Float32})::VecOrMat{Int} where {T}
    probabilities, logprobabilities = get_probs_logprobs(p, ùê¨)
    if ndims(probabilities) == 3
        n, seq_len, batch_size = size(probabilities)
        ùêö = zeros(Int, seq_len, batch_size)
        for i in 1:batch_size
            for j in 1:seq_len
                ùêö[j, i] = sample(rng, 1:n, ProbabilityWeights(probabilities[:, j, i]))
            end
        end
    else
        n, batch_size = size(probabilities)
        ùêö = zeros(Int, batch_size)
        for i in 1:batch_size
            ùêö[i] = sample(rng, 1:n, ProbabilityWeights(probabilities[:, i]))
        end
    end
    return ùêö
end

function get_probs_logprobs(p::PPOActorDiscrete{T}, ùê¨::AbstractArray{Float32})::Tuple{AbstractArray{Float32}, AbstractArray{Float32}} where {T}
    if p.recurtype ‚àà (MARKOV, TRANSFORMER) || ndims(ùê¨) == 2
        logits = p.actor_model(ùê¨)
    else
        # interpret as (state_dim, ntimesteps, batch_size) for the RNN
        Flux.Zygote.@ignore Flux.reset!(p.actor_model)
        logits = mapfoldl(hcat, 1:size(ùê¨, 2)) do t
            return reshape(p.actor_model(ùê¨[:, t, :]), :, 1, size(ùê¨, 3))
        end
    end
    if p.deterministic
        logits = logits * 1.0f6
    end
    probabilities = Flux.softmax(logits)
    logprobabilities = Flux.logsoftmax(logits)
    return probabilities, logprobabilities
end

function get_entropy(p::PPOActorDiscrete{T}, ùê¨::AbstractArray{Float32})::Float32 where {T}
    ùõë, logùõë = get_probs_logprobs(p, ùê¨)
    return -sum(ùõë .* logùõë; dims=1) |> mean
end

function get_entropy(p::PPOActorDiscrete{T}, ùõë, logùõë)::Float32 where {T}
    return -sum(ùõë .* logùõë; dims=1) |> mean
end

function get_kl_div(p::PPOActorDiscrete, ùê¨, ùêö, oldùõë, oldlogùõë)
    ùõë, logùõë = get_probs_logprobs(p, ùê¨)
    return sum(oldùõë .* (oldlogùõë .- logùõë); dims=1) |> mean
end


function get_loss_and_entropy(p::PPOActorDiscrete, ùê¨, ùêö, ùõÖ, oldùõë, oldlogùõë, œµ, use_clip_objective=true)
    _, seq_len, batch_size = size(ùêö)
    ùêö = Flux.Zygote.@ignore [CartesianIndex(ùêö[1, t, i], t, i) for t in 1:seq_len, i in 1:batch_size]
    ùêö = reshape(ùêö, 1, seq_len, batch_size)
    ùõë, logùõë = get_probs_logprobs(p, ùê¨)
    if use_clip_objective
        ùëü =  ùõë[ùêö] ./ oldùõë[ùêö]
        loss = -min.(ùëü .* ùõÖ, clamp.(ùëü, 1f0-œµ, 1f0+œµ) .* ùõÖ) |> mean
    else
        loss = -ùõÖ .* logùõë[ùêö] |> mean
    end
    entropy = get_entropy(p, ùõë, logùõë)
    return loss, entropy
end







mutable struct PPOActorContinuous{T‚Çõ <: AbstractFloat, T‚Çê <: AbstractFloat} <: AbstractPolicy{Vector{T‚Çõ}, Vector{T‚Çê}}
    const recurtype::RecurrenceType
    actor_model  # maps states to action log probabilities
    deterministic::Bool
    
    logstd::AbstractVector{Float32}
    shift::AbstractVector{Float32}
    scale::AbstractVector{Float32}

    const observation_history::Vector{Vector{Float32}}
    const misc_args
end
function PPOActorContinuous{T‚Çõ, T‚Çê}(recurtype::RecurrenceType, actor_model, deterministic::Bool, aspace::VectorSpace{T‚Çê}, misc_args) where {T‚Çõ, T‚Çê}
    n = size(aspace, 1)
    logstd = zeros(n) |> tof32
    shift = (aspace.lows + aspace.highs) / 2  |> tof32
    scale = (aspace.highs - aspace.lows) / 2  |> tof32
    return PPOActorContinuous{T‚Çõ, T‚Çê}(recurtype, actor_model, deterministic, logstd, shift, scale, Vector{Float32}[], misc_args)
end

Flux.@functor PPOActorContinuous (actor_model, logstd)
Flux.gpu(p::PPOActorContinuous{T‚Çõ, T‚Çê}) where {T‚Çõ, T‚Çê}  = PPOActorContinuous{T‚Çõ, T‚Çê}(p.recurtype, Flux.gpu(p.actor_model), p.deterministic, Flux.gpu(p.logstd), Flux.gpu(p.shift), Flux.gpu(p.scale), p.observation_history, p.misc_args)
Flux.cpu(p::PPOActorContinuous{T‚Çõ, T‚Çê}) where {T‚Çõ, T‚Çê}  = PPOActorContinuous{T‚Çõ, T‚Çê}(p.recurtype, Flux.cpu(p.actor_model), p.deterministic, Flux.cpu(p.logstd), Flux.cpu(p.shift), Flux.cpu(p.scale), p.observation_history, p.misc_args)


function (p::PPOActorContinuous{T‚Çõ, T‚Çê})(rng::AbstractRNG, ùê¨::AbstractArray{Float32})::AbstractArray{Float32} where {T‚Çõ, T‚Çê}
    ùêö, logùõëùêö = sample_action_logprobs(p, rng, ùê¨)
    return ùêö
end

function sample_action_logprobs(p::PPOActorContinuous{T‚Çõ, T‚Çê}, rng::AbstractRNG, ùê¨::AbstractArray{Float32})::Tuple{AbstractArray{Float32}, AbstractArray{Float32}} where {T‚Çõ, T‚Çê}
    ùõç = p.actor_model(ùê¨)
    logùõî = clamp.(p.logstd, -20f0, 2f0)
    ùõî = (1f0 - Float32(p.deterministic)) * exp.(logùõî)
    ùõè = Flux.Zygote.@ignore convert(typeof(ùõç), randn(rng, Float32, size(ùõç)))
    ùêö = ùõç .+ ùõî .* ùõè
    logùõëùêö = sum(log_nomal_prob.(ùêö, ùõç, ùõî), dims=1)
    ùêö = tanh.(ùêö)
    logùõëùêö = logùõëùêö .- sum(log.(1f0 .- ùêö .^ 2 .+ 1f-6), dims=1)
    ùêö = p.shift .+ p.scale .* ùêö
    return ùêö, logùõëùêö
end


function get_logprobs(p::PPOActorContinuous{T‚Çõ, T‚Çê}, ùê¨::AbstractArray{Float32}, ùêö::AbstractArray{Float32})::AbstractArray{Float32} where {T‚Çõ, T‚Çê}
    ùêö = clamp.(ùêö, -1f0 + 1f-6, 1f0 - 1f-6)
    ùêö_unshifted_unscaled = (ùêö .- p.shift) ./ p.scale
    ùêö_untanhed = atanh.(ùêö_unshifted_unscaled)

    if p.recurtype ‚àà (MARKOV, TRANSFORMER) || ndims(ùê¨) == 2
        ùõç = p.actor_model(ùê¨)
    else
        # interpret as (state_dim, ntimesteps, batch_size)
        Flux.Zygote.@ignore Flux.reset!(p.actor_model)
        ùõç = mapfoldl(hcat, 1:size(ùê¨, 2)) do t
            reshape(p.actor_model(ùê¨[:, t, :]), 1, 1, size(ùê¨, 3))
        end
    end
    ùõç = p.actor_model(ùê¨)
    logùõî = clamp.(p.logstd, -20f0, 2f0)
    ùõî = (1f0 - Float32(p.deterministic)) * exp.(logùõî)

    logùõëùêö = sum(log_nomal_prob.(ùêö_untanhed, ùõç, ùõî), dims=1)
    logùõëùêö = logùõëùêö .- sum(log.(1f0 .- ùêö_unshifted_unscaled .^ 2 .+ 1f-6), dims=1)

    return logùõëùêö
end

function get_entropy(p::PPOActorContinuous)
    D = length(p.logstd)
    logœÉ = clamp.(p.logstd, -20f0, 2f0)
    return 0.5f0 * D * (1f0 + LOG_2PI) + sum(logœÉ)
end

function get_entropy(p::PPOActorContinuous, ùê¨)
    get_entropy(p)
end

function get_kl_div(p::PPOActorContinuous, ùê¨, ùêö, oldùõë, oldlogùõë)
    logùõë = get_logprobs(p, ùê¨, ùêö)
    logratio = logùõë .- oldlogùõë
    kl = (exp.(logratio) .- 1f0 .- logratio) |> mean  # More stable than -logratio. http://joschu.net/blog/kl-approx.html
    return kl
end


function get_loss_and_entropy(p::PPOActorContinuous, ùê¨, ùêö, ùõÖ, oldùõë, oldlogùõë, œµ, use_clip_objective=true)
    logùõë = get_logprobs(p, ùê¨, ùêö)
    ùëü = exp.(logùõë .- oldlogùõë)
    if use_clip_objective
        loss = -min.(ùëü .* ùõÖ, clamp.(ùëü, 1f0 - œµ, 1f0 + œµ) .* ùõÖ) |> mean
    else
        loss = ùõÖ .* -logùõë |> mean
    end
    entropy = get_entropy(p)
    return loss, entropy
end





const PPOActor{T‚Çõ, T‚Çê} = Union{PPOActorContinuous{T‚Çõ, T‚Çê}, PPOActorDiscrete{T‚Çõ}}


function MDPs.preepisode(p::PPOActor; env, kwargs...)
    Flux.reset!(p.actor_model)
    empty!(p.observation_history)
    p.recurtype == TRANSFORMER && push!(p.observation_history, tof32(deepcopy(state(env))))
    nothing
end

function (p::PPOActorDiscrete{T‚Çõ})(rng::AbstractRNG, s::Vector{T‚Çõ})::Int where {T‚Çõ}
    ppo_unified(p, rng, s)
end
function (p::PPOActorContinuous{T‚Çõ, T‚Çê})(rng::AbstractRNG, s::Vector{T‚Çõ})::Vector{T‚Çê} where {T‚Çõ, T‚Çê}
    ppo_unified(p, rng, s)
end

function ppo_unified(p::PPOActor{T‚Çõ, T‚Çê}, rng::AbstractRNG, s::Vector{T‚Çõ}) where {T‚Çõ, T‚Çê}
    if p.recurtype == TRANSFORMER
        push!(p.observation_history, tof32(deepcopy(s)))
        s = hcat(p.observation_history...)
        # some ugly stuff: TODO: put this into a function or some wrapper.
        # sl = size(s, 2)
        # task_horizon, horizon, extrapolate_to = p.misc_args
        # _CL = decide_context_length(sl, extrapolate_to, horizon, task_horizon)
        # if sl > _CL
        #     if sl % task_horizon == 0
        #         cl = _CL
        #     else
        #         cl = _CL - task_horizon + sl % task_horizon
        #     end
        #     s = s[:, end-cl+1:end]
        #     tstart = s[end, 1]
        #     s[end, :] = s[end, :] .- tstart
        #     s[end, :] = (extrapolate_to/horizon) * s[end, :]
        # end
        # end of ugly stuff.
    end
    ùê¨ = s |> batch |> tof32
    a = p(rng, ùê¨) |> unbatch
    if p.recurtype == TRANSFORMER
        a = unbatch_last(a)
    end
    return a
end

